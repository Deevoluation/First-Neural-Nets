{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on wheat seed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('seeds_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.26</td>\n",
       "      <td>14.84</td>\n",
       "      <td>0.8710</td>\n",
       "      <td>5.763</td>\n",
       "      <td>3.312</td>\n",
       "      <td>2.2210</td>\n",
       "      <td>5.220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.88</td>\n",
       "      <td>14.57</td>\n",
       "      <td>0.8811</td>\n",
       "      <td>5.554</td>\n",
       "      <td>3.333</td>\n",
       "      <td>1.0180</td>\n",
       "      <td>4.956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.29</td>\n",
       "      <td>14.09</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>5.291</td>\n",
       "      <td>3.337</td>\n",
       "      <td>2.6990</td>\n",
       "      <td>4.825</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.84</td>\n",
       "      <td>13.94</td>\n",
       "      <td>0.8955</td>\n",
       "      <td>5.324</td>\n",
       "      <td>3.379</td>\n",
       "      <td>2.2590</td>\n",
       "      <td>4.805</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.14</td>\n",
       "      <td>14.99</td>\n",
       "      <td>0.9034</td>\n",
       "      <td>5.658</td>\n",
       "      <td>3.562</td>\n",
       "      <td>1.3550</td>\n",
       "      <td>5.175</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.38</td>\n",
       "      <td>14.21</td>\n",
       "      <td>0.8951</td>\n",
       "      <td>5.386</td>\n",
       "      <td>3.312</td>\n",
       "      <td>2.4620</td>\n",
       "      <td>4.956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.69</td>\n",
       "      <td>14.49</td>\n",
       "      <td>0.8799</td>\n",
       "      <td>5.563</td>\n",
       "      <td>3.259</td>\n",
       "      <td>3.5860</td>\n",
       "      <td>5.219</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.11</td>\n",
       "      <td>14.10</td>\n",
       "      <td>0.8911</td>\n",
       "      <td>5.420</td>\n",
       "      <td>3.302</td>\n",
       "      <td>2.7000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16.63</td>\n",
       "      <td>15.46</td>\n",
       "      <td>0.8747</td>\n",
       "      <td>6.053</td>\n",
       "      <td>3.465</td>\n",
       "      <td>2.0400</td>\n",
       "      <td>5.877</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16.44</td>\n",
       "      <td>15.25</td>\n",
       "      <td>0.8880</td>\n",
       "      <td>5.884</td>\n",
       "      <td>3.505</td>\n",
       "      <td>1.9690</td>\n",
       "      <td>5.533</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15.26</td>\n",
       "      <td>14.85</td>\n",
       "      <td>0.8696</td>\n",
       "      <td>5.714</td>\n",
       "      <td>3.242</td>\n",
       "      <td>4.5430</td>\n",
       "      <td>5.314</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14.03</td>\n",
       "      <td>14.16</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>5.438</td>\n",
       "      <td>3.201</td>\n",
       "      <td>1.7170</td>\n",
       "      <td>5.001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.89</td>\n",
       "      <td>14.02</td>\n",
       "      <td>0.8880</td>\n",
       "      <td>5.439</td>\n",
       "      <td>3.199</td>\n",
       "      <td>3.9860</td>\n",
       "      <td>4.738</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13.78</td>\n",
       "      <td>14.06</td>\n",
       "      <td>0.8759</td>\n",
       "      <td>5.479</td>\n",
       "      <td>3.156</td>\n",
       "      <td>3.1360</td>\n",
       "      <td>4.872</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13.74</td>\n",
       "      <td>14.05</td>\n",
       "      <td>0.8744</td>\n",
       "      <td>5.482</td>\n",
       "      <td>3.114</td>\n",
       "      <td>2.9320</td>\n",
       "      <td>4.825</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14.59</td>\n",
       "      <td>14.28</td>\n",
       "      <td>0.8993</td>\n",
       "      <td>5.351</td>\n",
       "      <td>3.333</td>\n",
       "      <td>4.1850</td>\n",
       "      <td>4.781</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>13.99</td>\n",
       "      <td>13.83</td>\n",
       "      <td>0.9183</td>\n",
       "      <td>5.119</td>\n",
       "      <td>3.383</td>\n",
       "      <td>5.2340</td>\n",
       "      <td>4.781</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>15.69</td>\n",
       "      <td>14.75</td>\n",
       "      <td>0.9058</td>\n",
       "      <td>5.527</td>\n",
       "      <td>3.514</td>\n",
       "      <td>1.5990</td>\n",
       "      <td>5.046</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14.70</td>\n",
       "      <td>14.21</td>\n",
       "      <td>0.9153</td>\n",
       "      <td>5.205</td>\n",
       "      <td>3.466</td>\n",
       "      <td>1.7670</td>\n",
       "      <td>4.649</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>12.72</td>\n",
       "      <td>13.57</td>\n",
       "      <td>0.8686</td>\n",
       "      <td>5.226</td>\n",
       "      <td>3.049</td>\n",
       "      <td>4.1020</td>\n",
       "      <td>4.914</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14.16</td>\n",
       "      <td>14.40</td>\n",
       "      <td>0.8584</td>\n",
       "      <td>5.658</td>\n",
       "      <td>3.129</td>\n",
       "      <td>3.0720</td>\n",
       "      <td>5.176</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>14.11</td>\n",
       "      <td>14.26</td>\n",
       "      <td>0.8722</td>\n",
       "      <td>5.520</td>\n",
       "      <td>3.168</td>\n",
       "      <td>2.6880</td>\n",
       "      <td>5.219</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.88</td>\n",
       "      <td>14.90</td>\n",
       "      <td>0.8988</td>\n",
       "      <td>5.618</td>\n",
       "      <td>3.507</td>\n",
       "      <td>0.7651</td>\n",
       "      <td>5.091</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>12.08</td>\n",
       "      <td>13.23</td>\n",
       "      <td>0.8664</td>\n",
       "      <td>5.099</td>\n",
       "      <td>2.936</td>\n",
       "      <td>1.4150</td>\n",
       "      <td>4.961</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>15.01</td>\n",
       "      <td>14.76</td>\n",
       "      <td>0.8657</td>\n",
       "      <td>5.789</td>\n",
       "      <td>3.245</td>\n",
       "      <td>1.7910</td>\n",
       "      <td>5.001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16.19</td>\n",
       "      <td>15.16</td>\n",
       "      <td>0.8849</td>\n",
       "      <td>5.833</td>\n",
       "      <td>3.421</td>\n",
       "      <td>0.9030</td>\n",
       "      <td>5.307</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>13.02</td>\n",
       "      <td>13.76</td>\n",
       "      <td>0.8641</td>\n",
       "      <td>5.395</td>\n",
       "      <td>3.026</td>\n",
       "      <td>3.3730</td>\n",
       "      <td>4.825</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>12.74</td>\n",
       "      <td>13.67</td>\n",
       "      <td>0.8564</td>\n",
       "      <td>5.395</td>\n",
       "      <td>2.956</td>\n",
       "      <td>2.5040</td>\n",
       "      <td>4.869</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>14.11</td>\n",
       "      <td>14.18</td>\n",
       "      <td>0.8820</td>\n",
       "      <td>5.541</td>\n",
       "      <td>3.221</td>\n",
       "      <td>2.7540</td>\n",
       "      <td>5.038</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>13.45</td>\n",
       "      <td>14.02</td>\n",
       "      <td>0.8604</td>\n",
       "      <td>5.516</td>\n",
       "      <td>3.065</td>\n",
       "      <td>3.5310</td>\n",
       "      <td>5.097</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>11.41</td>\n",
       "      <td>12.95</td>\n",
       "      <td>0.8560</td>\n",
       "      <td>5.090</td>\n",
       "      <td>2.775</td>\n",
       "      <td>4.9570</td>\n",
       "      <td>4.825</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>12.46</td>\n",
       "      <td>13.41</td>\n",
       "      <td>0.8706</td>\n",
       "      <td>5.236</td>\n",
       "      <td>3.017</td>\n",
       "      <td>4.9870</td>\n",
       "      <td>5.147</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>12.19</td>\n",
       "      <td>13.36</td>\n",
       "      <td>0.8579</td>\n",
       "      <td>5.240</td>\n",
       "      <td>2.909</td>\n",
       "      <td>4.8570</td>\n",
       "      <td>5.158</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>11.65</td>\n",
       "      <td>13.07</td>\n",
       "      <td>0.8575</td>\n",
       "      <td>5.108</td>\n",
       "      <td>2.850</td>\n",
       "      <td>5.2090</td>\n",
       "      <td>5.135</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>12.89</td>\n",
       "      <td>13.77</td>\n",
       "      <td>0.8541</td>\n",
       "      <td>5.495</td>\n",
       "      <td>3.026</td>\n",
       "      <td>6.1850</td>\n",
       "      <td>5.316</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>11.56</td>\n",
       "      <td>13.31</td>\n",
       "      <td>0.8198</td>\n",
       "      <td>5.363</td>\n",
       "      <td>2.683</td>\n",
       "      <td>4.0620</td>\n",
       "      <td>5.182</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>11.81</td>\n",
       "      <td>13.45</td>\n",
       "      <td>0.8198</td>\n",
       "      <td>5.413</td>\n",
       "      <td>2.716</td>\n",
       "      <td>4.8980</td>\n",
       "      <td>5.352</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>10.91</td>\n",
       "      <td>12.80</td>\n",
       "      <td>0.8372</td>\n",
       "      <td>5.088</td>\n",
       "      <td>2.675</td>\n",
       "      <td>4.1790</td>\n",
       "      <td>4.956</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>11.23</td>\n",
       "      <td>12.82</td>\n",
       "      <td>0.8594</td>\n",
       "      <td>5.089</td>\n",
       "      <td>2.821</td>\n",
       "      <td>7.5240</td>\n",
       "      <td>4.957</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>10.59</td>\n",
       "      <td>12.41</td>\n",
       "      <td>0.8648</td>\n",
       "      <td>4.899</td>\n",
       "      <td>2.787</td>\n",
       "      <td>4.9750</td>\n",
       "      <td>4.794</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>10.93</td>\n",
       "      <td>12.80</td>\n",
       "      <td>0.8390</td>\n",
       "      <td>5.046</td>\n",
       "      <td>2.717</td>\n",
       "      <td>5.3980</td>\n",
       "      <td>5.045</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>11.27</td>\n",
       "      <td>12.86</td>\n",
       "      <td>0.8563</td>\n",
       "      <td>5.091</td>\n",
       "      <td>2.804</td>\n",
       "      <td>3.9850</td>\n",
       "      <td>5.001</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>11.87</td>\n",
       "      <td>13.02</td>\n",
       "      <td>0.8795</td>\n",
       "      <td>5.132</td>\n",
       "      <td>2.953</td>\n",
       "      <td>3.5970</td>\n",
       "      <td>5.132</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>10.82</td>\n",
       "      <td>12.83</td>\n",
       "      <td>0.8256</td>\n",
       "      <td>5.180</td>\n",
       "      <td>2.630</td>\n",
       "      <td>4.8530</td>\n",
       "      <td>5.089</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>12.11</td>\n",
       "      <td>13.27</td>\n",
       "      <td>0.8639</td>\n",
       "      <td>5.236</td>\n",
       "      <td>2.975</td>\n",
       "      <td>4.1320</td>\n",
       "      <td>5.012</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>12.80</td>\n",
       "      <td>13.47</td>\n",
       "      <td>0.8860</td>\n",
       "      <td>5.160</td>\n",
       "      <td>3.126</td>\n",
       "      <td>4.8730</td>\n",
       "      <td>4.914</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>12.79</td>\n",
       "      <td>13.53</td>\n",
       "      <td>0.8786</td>\n",
       "      <td>5.224</td>\n",
       "      <td>3.054</td>\n",
       "      <td>5.4830</td>\n",
       "      <td>4.958</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>13.37</td>\n",
       "      <td>13.78</td>\n",
       "      <td>0.8849</td>\n",
       "      <td>5.320</td>\n",
       "      <td>3.128</td>\n",
       "      <td>4.6700</td>\n",
       "      <td>5.091</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>12.62</td>\n",
       "      <td>13.67</td>\n",
       "      <td>0.8481</td>\n",
       "      <td>5.410</td>\n",
       "      <td>2.911</td>\n",
       "      <td>3.3060</td>\n",
       "      <td>5.231</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>12.76</td>\n",
       "      <td>13.38</td>\n",
       "      <td>0.8964</td>\n",
       "      <td>5.073</td>\n",
       "      <td>3.155</td>\n",
       "      <td>2.8280</td>\n",
       "      <td>4.830</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>12.38</td>\n",
       "      <td>13.44</td>\n",
       "      <td>0.8609</td>\n",
       "      <td>5.219</td>\n",
       "      <td>2.989</td>\n",
       "      <td>5.4720</td>\n",
       "      <td>5.045</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>12.67</td>\n",
       "      <td>13.32</td>\n",
       "      <td>0.8977</td>\n",
       "      <td>4.984</td>\n",
       "      <td>3.135</td>\n",
       "      <td>2.3000</td>\n",
       "      <td>4.745</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>11.18</td>\n",
       "      <td>12.72</td>\n",
       "      <td>0.8680</td>\n",
       "      <td>5.009</td>\n",
       "      <td>2.810</td>\n",
       "      <td>4.0510</td>\n",
       "      <td>4.828</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>12.70</td>\n",
       "      <td>13.41</td>\n",
       "      <td>0.8874</td>\n",
       "      <td>5.183</td>\n",
       "      <td>3.091</td>\n",
       "      <td>8.4560</td>\n",
       "      <td>5.000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>12.37</td>\n",
       "      <td>13.47</td>\n",
       "      <td>0.8567</td>\n",
       "      <td>5.204</td>\n",
       "      <td>2.960</td>\n",
       "      <td>3.9190</td>\n",
       "      <td>5.001</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>12.19</td>\n",
       "      <td>13.20</td>\n",
       "      <td>0.8783</td>\n",
       "      <td>5.137</td>\n",
       "      <td>2.981</td>\n",
       "      <td>3.6310</td>\n",
       "      <td>4.870</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>11.23</td>\n",
       "      <td>12.88</td>\n",
       "      <td>0.8511</td>\n",
       "      <td>5.140</td>\n",
       "      <td>2.795</td>\n",
       "      <td>4.3250</td>\n",
       "      <td>5.003</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>13.20</td>\n",
       "      <td>13.66</td>\n",
       "      <td>0.8883</td>\n",
       "      <td>5.236</td>\n",
       "      <td>3.232</td>\n",
       "      <td>8.3150</td>\n",
       "      <td>5.056</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>11.84</td>\n",
       "      <td>13.21</td>\n",
       "      <td>0.8521</td>\n",
       "      <td>5.175</td>\n",
       "      <td>2.836</td>\n",
       "      <td>3.5980</td>\n",
       "      <td>5.044</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>12.30</td>\n",
       "      <td>13.34</td>\n",
       "      <td>0.8684</td>\n",
       "      <td>5.243</td>\n",
       "      <td>2.974</td>\n",
       "      <td>5.6370</td>\n",
       "      <td>5.063</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        V1     V2      V3     V4     V5      V6     V7  V8\n",
       "0    15.26  14.84  0.8710  5.763  3.312  2.2210  5.220   1\n",
       "1    14.88  14.57  0.8811  5.554  3.333  1.0180  4.956   1\n",
       "2    14.29  14.09  0.9050  5.291  3.337  2.6990  4.825   1\n",
       "3    13.84  13.94  0.8955  5.324  3.379  2.2590  4.805   1\n",
       "4    16.14  14.99  0.9034  5.658  3.562  1.3550  5.175   1\n",
       "5    14.38  14.21  0.8951  5.386  3.312  2.4620  4.956   1\n",
       "6    14.69  14.49  0.8799  5.563  3.259  3.5860  5.219   1\n",
       "7    14.11  14.10  0.8911  5.420  3.302  2.7000  5.000   1\n",
       "8    16.63  15.46  0.8747  6.053  3.465  2.0400  5.877   1\n",
       "9    16.44  15.25  0.8880  5.884  3.505  1.9690  5.533   1\n",
       "10   15.26  14.85  0.8696  5.714  3.242  4.5430  5.314   1\n",
       "11   14.03  14.16  0.8796  5.438  3.201  1.7170  5.001   1\n",
       "12   13.89  14.02  0.8880  5.439  3.199  3.9860  4.738   1\n",
       "13   13.78  14.06  0.8759  5.479  3.156  3.1360  4.872   1\n",
       "14   13.74  14.05  0.8744  5.482  3.114  2.9320  4.825   1\n",
       "15   14.59  14.28  0.8993  5.351  3.333  4.1850  4.781   1\n",
       "16   13.99  13.83  0.9183  5.119  3.383  5.2340  4.781   1\n",
       "17   15.69  14.75  0.9058  5.527  3.514  1.5990  5.046   1\n",
       "18   14.70  14.21  0.9153  5.205  3.466  1.7670  4.649   1\n",
       "19   12.72  13.57  0.8686  5.226  3.049  4.1020  4.914   1\n",
       "20   14.16  14.40  0.8584  5.658  3.129  3.0720  5.176   1\n",
       "21   14.11  14.26  0.8722  5.520  3.168  2.6880  5.219   1\n",
       "22   15.88  14.90  0.8988  5.618  3.507  0.7651  5.091   1\n",
       "23   12.08  13.23  0.8664  5.099  2.936  1.4150  4.961   1\n",
       "24   15.01  14.76  0.8657  5.789  3.245  1.7910  5.001   1\n",
       "25   16.19  15.16  0.8849  5.833  3.421  0.9030  5.307   1\n",
       "26   13.02  13.76  0.8641  5.395  3.026  3.3730  4.825   1\n",
       "27   12.74  13.67  0.8564  5.395  2.956  2.5040  4.869   1\n",
       "28   14.11  14.18  0.8820  5.541  3.221  2.7540  5.038   1\n",
       "29   13.45  14.02  0.8604  5.516  3.065  3.5310  5.097   1\n",
       "..     ...    ...     ...    ...    ...     ...    ...  ..\n",
       "180  11.41  12.95  0.8560  5.090  2.775  4.9570  4.825   3\n",
       "181  12.46  13.41  0.8706  5.236  3.017  4.9870  5.147   3\n",
       "182  12.19  13.36  0.8579  5.240  2.909  4.8570  5.158   3\n",
       "183  11.65  13.07  0.8575  5.108  2.850  5.2090  5.135   3\n",
       "184  12.89  13.77  0.8541  5.495  3.026  6.1850  5.316   3\n",
       "185  11.56  13.31  0.8198  5.363  2.683  4.0620  5.182   3\n",
       "186  11.81  13.45  0.8198  5.413  2.716  4.8980  5.352   3\n",
       "187  10.91  12.80  0.8372  5.088  2.675  4.1790  4.956   3\n",
       "188  11.23  12.82  0.8594  5.089  2.821  7.5240  4.957   3\n",
       "189  10.59  12.41  0.8648  4.899  2.787  4.9750  4.794   3\n",
       "190  10.93  12.80  0.8390  5.046  2.717  5.3980  5.045   3\n",
       "191  11.27  12.86  0.8563  5.091  2.804  3.9850  5.001   3\n",
       "192  11.87  13.02  0.8795  5.132  2.953  3.5970  5.132   3\n",
       "193  10.82  12.83  0.8256  5.180  2.630  4.8530  5.089   3\n",
       "194  12.11  13.27  0.8639  5.236  2.975  4.1320  5.012   3\n",
       "195  12.80  13.47  0.8860  5.160  3.126  4.8730  4.914   3\n",
       "196  12.79  13.53  0.8786  5.224  3.054  5.4830  4.958   3\n",
       "197  13.37  13.78  0.8849  5.320  3.128  4.6700  5.091   3\n",
       "198  12.62  13.67  0.8481  5.410  2.911  3.3060  5.231   3\n",
       "199  12.76  13.38  0.8964  5.073  3.155  2.8280  4.830   3\n",
       "200  12.38  13.44  0.8609  5.219  2.989  5.4720  5.045   3\n",
       "201  12.67  13.32  0.8977  4.984  3.135  2.3000  4.745   3\n",
       "202  11.18  12.72  0.8680  5.009  2.810  4.0510  4.828   3\n",
       "203  12.70  13.41  0.8874  5.183  3.091  8.4560  5.000   3\n",
       "204  12.37  13.47  0.8567  5.204  2.960  3.9190  5.001   3\n",
       "205  12.19  13.20  0.8783  5.137  2.981  3.6310  4.870   3\n",
       "206  11.23  12.88  0.8511  5.140  2.795  4.3250  5.003   3\n",
       "207  13.20  13.66  0.8883  5.236  3.232  8.3150  5.056   3\n",
       "208  11.84  13.21  0.8521  5.175  2.836  3.5980  5.044   3\n",
       "209  12.30  13.34  0.8684  5.243  2.974  5.6370  5.063   3\n",
       "\n",
       "[210 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First clean the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = data.columns\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = data[col[:7]]\n",
    "y = data[col[7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.26</td>\n",
       "      <td>14.84</td>\n",
       "      <td>0.8710</td>\n",
       "      <td>5.763</td>\n",
       "      <td>3.312</td>\n",
       "      <td>2.2210</td>\n",
       "      <td>5.220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.88</td>\n",
       "      <td>14.57</td>\n",
       "      <td>0.8811</td>\n",
       "      <td>5.554</td>\n",
       "      <td>3.333</td>\n",
       "      <td>1.0180</td>\n",
       "      <td>4.956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.29</td>\n",
       "      <td>14.09</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>5.291</td>\n",
       "      <td>3.337</td>\n",
       "      <td>2.6990</td>\n",
       "      <td>4.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.84</td>\n",
       "      <td>13.94</td>\n",
       "      <td>0.8955</td>\n",
       "      <td>5.324</td>\n",
       "      <td>3.379</td>\n",
       "      <td>2.2590</td>\n",
       "      <td>4.805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.14</td>\n",
       "      <td>14.99</td>\n",
       "      <td>0.9034</td>\n",
       "      <td>5.658</td>\n",
       "      <td>3.562</td>\n",
       "      <td>1.3550</td>\n",
       "      <td>5.175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.38</td>\n",
       "      <td>14.21</td>\n",
       "      <td>0.8951</td>\n",
       "      <td>5.386</td>\n",
       "      <td>3.312</td>\n",
       "      <td>2.4620</td>\n",
       "      <td>4.956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.69</td>\n",
       "      <td>14.49</td>\n",
       "      <td>0.8799</td>\n",
       "      <td>5.563</td>\n",
       "      <td>3.259</td>\n",
       "      <td>3.5860</td>\n",
       "      <td>5.219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.11</td>\n",
       "      <td>14.10</td>\n",
       "      <td>0.8911</td>\n",
       "      <td>5.420</td>\n",
       "      <td>3.302</td>\n",
       "      <td>2.7000</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16.63</td>\n",
       "      <td>15.46</td>\n",
       "      <td>0.8747</td>\n",
       "      <td>6.053</td>\n",
       "      <td>3.465</td>\n",
       "      <td>2.0400</td>\n",
       "      <td>5.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16.44</td>\n",
       "      <td>15.25</td>\n",
       "      <td>0.8880</td>\n",
       "      <td>5.884</td>\n",
       "      <td>3.505</td>\n",
       "      <td>1.9690</td>\n",
       "      <td>5.533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15.26</td>\n",
       "      <td>14.85</td>\n",
       "      <td>0.8696</td>\n",
       "      <td>5.714</td>\n",
       "      <td>3.242</td>\n",
       "      <td>4.5430</td>\n",
       "      <td>5.314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14.03</td>\n",
       "      <td>14.16</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>5.438</td>\n",
       "      <td>3.201</td>\n",
       "      <td>1.7170</td>\n",
       "      <td>5.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.89</td>\n",
       "      <td>14.02</td>\n",
       "      <td>0.8880</td>\n",
       "      <td>5.439</td>\n",
       "      <td>3.199</td>\n",
       "      <td>3.9860</td>\n",
       "      <td>4.738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13.78</td>\n",
       "      <td>14.06</td>\n",
       "      <td>0.8759</td>\n",
       "      <td>5.479</td>\n",
       "      <td>3.156</td>\n",
       "      <td>3.1360</td>\n",
       "      <td>4.872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13.74</td>\n",
       "      <td>14.05</td>\n",
       "      <td>0.8744</td>\n",
       "      <td>5.482</td>\n",
       "      <td>3.114</td>\n",
       "      <td>2.9320</td>\n",
       "      <td>4.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14.59</td>\n",
       "      <td>14.28</td>\n",
       "      <td>0.8993</td>\n",
       "      <td>5.351</td>\n",
       "      <td>3.333</td>\n",
       "      <td>4.1850</td>\n",
       "      <td>4.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>13.99</td>\n",
       "      <td>13.83</td>\n",
       "      <td>0.9183</td>\n",
       "      <td>5.119</td>\n",
       "      <td>3.383</td>\n",
       "      <td>5.2340</td>\n",
       "      <td>4.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>15.69</td>\n",
       "      <td>14.75</td>\n",
       "      <td>0.9058</td>\n",
       "      <td>5.527</td>\n",
       "      <td>3.514</td>\n",
       "      <td>1.5990</td>\n",
       "      <td>5.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14.70</td>\n",
       "      <td>14.21</td>\n",
       "      <td>0.9153</td>\n",
       "      <td>5.205</td>\n",
       "      <td>3.466</td>\n",
       "      <td>1.7670</td>\n",
       "      <td>4.649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>12.72</td>\n",
       "      <td>13.57</td>\n",
       "      <td>0.8686</td>\n",
       "      <td>5.226</td>\n",
       "      <td>3.049</td>\n",
       "      <td>4.1020</td>\n",
       "      <td>4.914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14.16</td>\n",
       "      <td>14.40</td>\n",
       "      <td>0.8584</td>\n",
       "      <td>5.658</td>\n",
       "      <td>3.129</td>\n",
       "      <td>3.0720</td>\n",
       "      <td>5.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>14.11</td>\n",
       "      <td>14.26</td>\n",
       "      <td>0.8722</td>\n",
       "      <td>5.520</td>\n",
       "      <td>3.168</td>\n",
       "      <td>2.6880</td>\n",
       "      <td>5.219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.88</td>\n",
       "      <td>14.90</td>\n",
       "      <td>0.8988</td>\n",
       "      <td>5.618</td>\n",
       "      <td>3.507</td>\n",
       "      <td>0.7651</td>\n",
       "      <td>5.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>12.08</td>\n",
       "      <td>13.23</td>\n",
       "      <td>0.8664</td>\n",
       "      <td>5.099</td>\n",
       "      <td>2.936</td>\n",
       "      <td>1.4150</td>\n",
       "      <td>4.961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>15.01</td>\n",
       "      <td>14.76</td>\n",
       "      <td>0.8657</td>\n",
       "      <td>5.789</td>\n",
       "      <td>3.245</td>\n",
       "      <td>1.7910</td>\n",
       "      <td>5.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16.19</td>\n",
       "      <td>15.16</td>\n",
       "      <td>0.8849</td>\n",
       "      <td>5.833</td>\n",
       "      <td>3.421</td>\n",
       "      <td>0.9030</td>\n",
       "      <td>5.307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>13.02</td>\n",
       "      <td>13.76</td>\n",
       "      <td>0.8641</td>\n",
       "      <td>5.395</td>\n",
       "      <td>3.026</td>\n",
       "      <td>3.3730</td>\n",
       "      <td>4.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>12.74</td>\n",
       "      <td>13.67</td>\n",
       "      <td>0.8564</td>\n",
       "      <td>5.395</td>\n",
       "      <td>2.956</td>\n",
       "      <td>2.5040</td>\n",
       "      <td>4.869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>14.11</td>\n",
       "      <td>14.18</td>\n",
       "      <td>0.8820</td>\n",
       "      <td>5.541</td>\n",
       "      <td>3.221</td>\n",
       "      <td>2.7540</td>\n",
       "      <td>5.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>13.45</td>\n",
       "      <td>14.02</td>\n",
       "      <td>0.8604</td>\n",
       "      <td>5.516</td>\n",
       "      <td>3.065</td>\n",
       "      <td>3.5310</td>\n",
       "      <td>5.097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>11.41</td>\n",
       "      <td>12.95</td>\n",
       "      <td>0.8560</td>\n",
       "      <td>5.090</td>\n",
       "      <td>2.775</td>\n",
       "      <td>4.9570</td>\n",
       "      <td>4.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>12.46</td>\n",
       "      <td>13.41</td>\n",
       "      <td>0.8706</td>\n",
       "      <td>5.236</td>\n",
       "      <td>3.017</td>\n",
       "      <td>4.9870</td>\n",
       "      <td>5.147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>12.19</td>\n",
       "      <td>13.36</td>\n",
       "      <td>0.8579</td>\n",
       "      <td>5.240</td>\n",
       "      <td>2.909</td>\n",
       "      <td>4.8570</td>\n",
       "      <td>5.158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>11.65</td>\n",
       "      <td>13.07</td>\n",
       "      <td>0.8575</td>\n",
       "      <td>5.108</td>\n",
       "      <td>2.850</td>\n",
       "      <td>5.2090</td>\n",
       "      <td>5.135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>12.89</td>\n",
       "      <td>13.77</td>\n",
       "      <td>0.8541</td>\n",
       "      <td>5.495</td>\n",
       "      <td>3.026</td>\n",
       "      <td>6.1850</td>\n",
       "      <td>5.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>11.56</td>\n",
       "      <td>13.31</td>\n",
       "      <td>0.8198</td>\n",
       "      <td>5.363</td>\n",
       "      <td>2.683</td>\n",
       "      <td>4.0620</td>\n",
       "      <td>5.182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>11.81</td>\n",
       "      <td>13.45</td>\n",
       "      <td>0.8198</td>\n",
       "      <td>5.413</td>\n",
       "      <td>2.716</td>\n",
       "      <td>4.8980</td>\n",
       "      <td>5.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>10.91</td>\n",
       "      <td>12.80</td>\n",
       "      <td>0.8372</td>\n",
       "      <td>5.088</td>\n",
       "      <td>2.675</td>\n",
       "      <td>4.1790</td>\n",
       "      <td>4.956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>11.23</td>\n",
       "      <td>12.82</td>\n",
       "      <td>0.8594</td>\n",
       "      <td>5.089</td>\n",
       "      <td>2.821</td>\n",
       "      <td>7.5240</td>\n",
       "      <td>4.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>10.59</td>\n",
       "      <td>12.41</td>\n",
       "      <td>0.8648</td>\n",
       "      <td>4.899</td>\n",
       "      <td>2.787</td>\n",
       "      <td>4.9750</td>\n",
       "      <td>4.794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>10.93</td>\n",
       "      <td>12.80</td>\n",
       "      <td>0.8390</td>\n",
       "      <td>5.046</td>\n",
       "      <td>2.717</td>\n",
       "      <td>5.3980</td>\n",
       "      <td>5.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>11.27</td>\n",
       "      <td>12.86</td>\n",
       "      <td>0.8563</td>\n",
       "      <td>5.091</td>\n",
       "      <td>2.804</td>\n",
       "      <td>3.9850</td>\n",
       "      <td>5.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>11.87</td>\n",
       "      <td>13.02</td>\n",
       "      <td>0.8795</td>\n",
       "      <td>5.132</td>\n",
       "      <td>2.953</td>\n",
       "      <td>3.5970</td>\n",
       "      <td>5.132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>10.82</td>\n",
       "      <td>12.83</td>\n",
       "      <td>0.8256</td>\n",
       "      <td>5.180</td>\n",
       "      <td>2.630</td>\n",
       "      <td>4.8530</td>\n",
       "      <td>5.089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>12.11</td>\n",
       "      <td>13.27</td>\n",
       "      <td>0.8639</td>\n",
       "      <td>5.236</td>\n",
       "      <td>2.975</td>\n",
       "      <td>4.1320</td>\n",
       "      <td>5.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>12.80</td>\n",
       "      <td>13.47</td>\n",
       "      <td>0.8860</td>\n",
       "      <td>5.160</td>\n",
       "      <td>3.126</td>\n",
       "      <td>4.8730</td>\n",
       "      <td>4.914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>12.79</td>\n",
       "      <td>13.53</td>\n",
       "      <td>0.8786</td>\n",
       "      <td>5.224</td>\n",
       "      <td>3.054</td>\n",
       "      <td>5.4830</td>\n",
       "      <td>4.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>13.37</td>\n",
       "      <td>13.78</td>\n",
       "      <td>0.8849</td>\n",
       "      <td>5.320</td>\n",
       "      <td>3.128</td>\n",
       "      <td>4.6700</td>\n",
       "      <td>5.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>12.62</td>\n",
       "      <td>13.67</td>\n",
       "      <td>0.8481</td>\n",
       "      <td>5.410</td>\n",
       "      <td>2.911</td>\n",
       "      <td>3.3060</td>\n",
       "      <td>5.231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>12.76</td>\n",
       "      <td>13.38</td>\n",
       "      <td>0.8964</td>\n",
       "      <td>5.073</td>\n",
       "      <td>3.155</td>\n",
       "      <td>2.8280</td>\n",
       "      <td>4.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>12.38</td>\n",
       "      <td>13.44</td>\n",
       "      <td>0.8609</td>\n",
       "      <td>5.219</td>\n",
       "      <td>2.989</td>\n",
       "      <td>5.4720</td>\n",
       "      <td>5.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>12.67</td>\n",
       "      <td>13.32</td>\n",
       "      <td>0.8977</td>\n",
       "      <td>4.984</td>\n",
       "      <td>3.135</td>\n",
       "      <td>2.3000</td>\n",
       "      <td>4.745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>11.18</td>\n",
       "      <td>12.72</td>\n",
       "      <td>0.8680</td>\n",
       "      <td>5.009</td>\n",
       "      <td>2.810</td>\n",
       "      <td>4.0510</td>\n",
       "      <td>4.828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>12.70</td>\n",
       "      <td>13.41</td>\n",
       "      <td>0.8874</td>\n",
       "      <td>5.183</td>\n",
       "      <td>3.091</td>\n",
       "      <td>8.4560</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>12.37</td>\n",
       "      <td>13.47</td>\n",
       "      <td>0.8567</td>\n",
       "      <td>5.204</td>\n",
       "      <td>2.960</td>\n",
       "      <td>3.9190</td>\n",
       "      <td>5.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>12.19</td>\n",
       "      <td>13.20</td>\n",
       "      <td>0.8783</td>\n",
       "      <td>5.137</td>\n",
       "      <td>2.981</td>\n",
       "      <td>3.6310</td>\n",
       "      <td>4.870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>11.23</td>\n",
       "      <td>12.88</td>\n",
       "      <td>0.8511</td>\n",
       "      <td>5.140</td>\n",
       "      <td>2.795</td>\n",
       "      <td>4.3250</td>\n",
       "      <td>5.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>13.20</td>\n",
       "      <td>13.66</td>\n",
       "      <td>0.8883</td>\n",
       "      <td>5.236</td>\n",
       "      <td>3.232</td>\n",
       "      <td>8.3150</td>\n",
       "      <td>5.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>11.84</td>\n",
       "      <td>13.21</td>\n",
       "      <td>0.8521</td>\n",
       "      <td>5.175</td>\n",
       "      <td>2.836</td>\n",
       "      <td>3.5980</td>\n",
       "      <td>5.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>12.30</td>\n",
       "      <td>13.34</td>\n",
       "      <td>0.8684</td>\n",
       "      <td>5.243</td>\n",
       "      <td>2.974</td>\n",
       "      <td>5.6370</td>\n",
       "      <td>5.063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        V1     V2      V3     V4     V5      V6     V7\n",
       "0    15.26  14.84  0.8710  5.763  3.312  2.2210  5.220\n",
       "1    14.88  14.57  0.8811  5.554  3.333  1.0180  4.956\n",
       "2    14.29  14.09  0.9050  5.291  3.337  2.6990  4.825\n",
       "3    13.84  13.94  0.8955  5.324  3.379  2.2590  4.805\n",
       "4    16.14  14.99  0.9034  5.658  3.562  1.3550  5.175\n",
       "5    14.38  14.21  0.8951  5.386  3.312  2.4620  4.956\n",
       "6    14.69  14.49  0.8799  5.563  3.259  3.5860  5.219\n",
       "7    14.11  14.10  0.8911  5.420  3.302  2.7000  5.000\n",
       "8    16.63  15.46  0.8747  6.053  3.465  2.0400  5.877\n",
       "9    16.44  15.25  0.8880  5.884  3.505  1.9690  5.533\n",
       "10   15.26  14.85  0.8696  5.714  3.242  4.5430  5.314\n",
       "11   14.03  14.16  0.8796  5.438  3.201  1.7170  5.001\n",
       "12   13.89  14.02  0.8880  5.439  3.199  3.9860  4.738\n",
       "13   13.78  14.06  0.8759  5.479  3.156  3.1360  4.872\n",
       "14   13.74  14.05  0.8744  5.482  3.114  2.9320  4.825\n",
       "15   14.59  14.28  0.8993  5.351  3.333  4.1850  4.781\n",
       "16   13.99  13.83  0.9183  5.119  3.383  5.2340  4.781\n",
       "17   15.69  14.75  0.9058  5.527  3.514  1.5990  5.046\n",
       "18   14.70  14.21  0.9153  5.205  3.466  1.7670  4.649\n",
       "19   12.72  13.57  0.8686  5.226  3.049  4.1020  4.914\n",
       "20   14.16  14.40  0.8584  5.658  3.129  3.0720  5.176\n",
       "21   14.11  14.26  0.8722  5.520  3.168  2.6880  5.219\n",
       "22   15.88  14.90  0.8988  5.618  3.507  0.7651  5.091\n",
       "23   12.08  13.23  0.8664  5.099  2.936  1.4150  4.961\n",
       "24   15.01  14.76  0.8657  5.789  3.245  1.7910  5.001\n",
       "25   16.19  15.16  0.8849  5.833  3.421  0.9030  5.307\n",
       "26   13.02  13.76  0.8641  5.395  3.026  3.3730  4.825\n",
       "27   12.74  13.67  0.8564  5.395  2.956  2.5040  4.869\n",
       "28   14.11  14.18  0.8820  5.541  3.221  2.7540  5.038\n",
       "29   13.45  14.02  0.8604  5.516  3.065  3.5310  5.097\n",
       "..     ...    ...     ...    ...    ...     ...    ...\n",
       "180  11.41  12.95  0.8560  5.090  2.775  4.9570  4.825\n",
       "181  12.46  13.41  0.8706  5.236  3.017  4.9870  5.147\n",
       "182  12.19  13.36  0.8579  5.240  2.909  4.8570  5.158\n",
       "183  11.65  13.07  0.8575  5.108  2.850  5.2090  5.135\n",
       "184  12.89  13.77  0.8541  5.495  3.026  6.1850  5.316\n",
       "185  11.56  13.31  0.8198  5.363  2.683  4.0620  5.182\n",
       "186  11.81  13.45  0.8198  5.413  2.716  4.8980  5.352\n",
       "187  10.91  12.80  0.8372  5.088  2.675  4.1790  4.956\n",
       "188  11.23  12.82  0.8594  5.089  2.821  7.5240  4.957\n",
       "189  10.59  12.41  0.8648  4.899  2.787  4.9750  4.794\n",
       "190  10.93  12.80  0.8390  5.046  2.717  5.3980  5.045\n",
       "191  11.27  12.86  0.8563  5.091  2.804  3.9850  5.001\n",
       "192  11.87  13.02  0.8795  5.132  2.953  3.5970  5.132\n",
       "193  10.82  12.83  0.8256  5.180  2.630  4.8530  5.089\n",
       "194  12.11  13.27  0.8639  5.236  2.975  4.1320  5.012\n",
       "195  12.80  13.47  0.8860  5.160  3.126  4.8730  4.914\n",
       "196  12.79  13.53  0.8786  5.224  3.054  5.4830  4.958\n",
       "197  13.37  13.78  0.8849  5.320  3.128  4.6700  5.091\n",
       "198  12.62  13.67  0.8481  5.410  2.911  3.3060  5.231\n",
       "199  12.76  13.38  0.8964  5.073  3.155  2.8280  4.830\n",
       "200  12.38  13.44  0.8609  5.219  2.989  5.4720  5.045\n",
       "201  12.67  13.32  0.8977  4.984  3.135  2.3000  4.745\n",
       "202  11.18  12.72  0.8680  5.009  2.810  4.0510  4.828\n",
       "203  12.70  13.41  0.8874  5.183  3.091  8.4560  5.000\n",
       "204  12.37  13.47  0.8567  5.204  2.960  3.9190  5.001\n",
       "205  12.19  13.20  0.8783  5.137  2.981  3.6310  4.870\n",
       "206  11.23  12.88  0.8511  5.140  2.795  4.3250  5.003\n",
       "207  13.20  13.66  0.8883  5.236  3.232  8.3150  5.056\n",
       "208  11.84  13.21  0.8521  5.175  2.836  3.5980  5.044\n",
       "209  12.30  13.34  0.8684  5.243  2.974  5.6370  5.063\n",
       "\n",
       "[210 rows x 7 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "5      1\n",
       "6      1\n",
       "7      1\n",
       "8      1\n",
       "9      1\n",
       "10     1\n",
       "11     1\n",
       "12     1\n",
       "13     1\n",
       "14     1\n",
       "15     1\n",
       "16     1\n",
       "17     1\n",
       "18     1\n",
       "19     1\n",
       "20     1\n",
       "21     1\n",
       "22     1\n",
       "23     1\n",
       "24     1\n",
       "25     1\n",
       "26     1\n",
       "27     1\n",
       "28     1\n",
       "29     1\n",
       "      ..\n",
       "180    3\n",
       "181    3\n",
       "182    3\n",
       "183    3\n",
       "184    3\n",
       "185    3\n",
       "186    3\n",
       "187    3\n",
       "188    3\n",
       "189    3\n",
       "190    3\n",
       "191    3\n",
       "192    3\n",
       "193    3\n",
       "194    3\n",
       "195    3\n",
       "196    3\n",
       "197    3\n",
       "198    3\n",
       "199    3\n",
       "200    3\n",
       "201    3\n",
       "202    3\n",
       "203    3\n",
       "204    3\n",
       "205    3\n",
       "206    3\n",
       "207    3\n",
       "208    3\n",
       "209    3\n",
       "Name: V8, Length: 210, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = LabelEncoder()\n",
    "y = onehot.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now here we Hotencode the labels\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y = y.reshape(len(y), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1    0\n",
      "V2    0\n",
      "V3    0\n",
      "V4    0\n",
      "V5    0\n",
      "V6    0\n",
      "V7    0\n",
      "V8    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.26</td>\n",
       "      <td>14.84</td>\n",
       "      <td>0.8710</td>\n",
       "      <td>5.763</td>\n",
       "      <td>3.312</td>\n",
       "      <td>2.2210</td>\n",
       "      <td>5.220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.88</td>\n",
       "      <td>14.57</td>\n",
       "      <td>0.8811</td>\n",
       "      <td>5.554</td>\n",
       "      <td>3.333</td>\n",
       "      <td>1.0180</td>\n",
       "      <td>4.956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.29</td>\n",
       "      <td>14.09</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>5.291</td>\n",
       "      <td>3.337</td>\n",
       "      <td>2.6990</td>\n",
       "      <td>4.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.84</td>\n",
       "      <td>13.94</td>\n",
       "      <td>0.8955</td>\n",
       "      <td>5.324</td>\n",
       "      <td>3.379</td>\n",
       "      <td>2.2590</td>\n",
       "      <td>4.805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.14</td>\n",
       "      <td>14.99</td>\n",
       "      <td>0.9034</td>\n",
       "      <td>5.658</td>\n",
       "      <td>3.562</td>\n",
       "      <td>1.3550</td>\n",
       "      <td>5.175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.38</td>\n",
       "      <td>14.21</td>\n",
       "      <td>0.8951</td>\n",
       "      <td>5.386</td>\n",
       "      <td>3.312</td>\n",
       "      <td>2.4620</td>\n",
       "      <td>4.956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.69</td>\n",
       "      <td>14.49</td>\n",
       "      <td>0.8799</td>\n",
       "      <td>5.563</td>\n",
       "      <td>3.259</td>\n",
       "      <td>3.5860</td>\n",
       "      <td>5.219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.11</td>\n",
       "      <td>14.10</td>\n",
       "      <td>0.8911</td>\n",
       "      <td>5.420</td>\n",
       "      <td>3.302</td>\n",
       "      <td>2.7000</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16.63</td>\n",
       "      <td>15.46</td>\n",
       "      <td>0.8747</td>\n",
       "      <td>6.053</td>\n",
       "      <td>3.465</td>\n",
       "      <td>2.0400</td>\n",
       "      <td>5.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16.44</td>\n",
       "      <td>15.25</td>\n",
       "      <td>0.8880</td>\n",
       "      <td>5.884</td>\n",
       "      <td>3.505</td>\n",
       "      <td>1.9690</td>\n",
       "      <td>5.533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15.26</td>\n",
       "      <td>14.85</td>\n",
       "      <td>0.8696</td>\n",
       "      <td>5.714</td>\n",
       "      <td>3.242</td>\n",
       "      <td>4.5430</td>\n",
       "      <td>5.314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14.03</td>\n",
       "      <td>14.16</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>5.438</td>\n",
       "      <td>3.201</td>\n",
       "      <td>1.7170</td>\n",
       "      <td>5.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.89</td>\n",
       "      <td>14.02</td>\n",
       "      <td>0.8880</td>\n",
       "      <td>5.439</td>\n",
       "      <td>3.199</td>\n",
       "      <td>3.9860</td>\n",
       "      <td>4.738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13.78</td>\n",
       "      <td>14.06</td>\n",
       "      <td>0.8759</td>\n",
       "      <td>5.479</td>\n",
       "      <td>3.156</td>\n",
       "      <td>3.1360</td>\n",
       "      <td>4.872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13.74</td>\n",
       "      <td>14.05</td>\n",
       "      <td>0.8744</td>\n",
       "      <td>5.482</td>\n",
       "      <td>3.114</td>\n",
       "      <td>2.9320</td>\n",
       "      <td>4.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14.59</td>\n",
       "      <td>14.28</td>\n",
       "      <td>0.8993</td>\n",
       "      <td>5.351</td>\n",
       "      <td>3.333</td>\n",
       "      <td>4.1850</td>\n",
       "      <td>4.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>13.99</td>\n",
       "      <td>13.83</td>\n",
       "      <td>0.9183</td>\n",
       "      <td>5.119</td>\n",
       "      <td>3.383</td>\n",
       "      <td>5.2340</td>\n",
       "      <td>4.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>15.69</td>\n",
       "      <td>14.75</td>\n",
       "      <td>0.9058</td>\n",
       "      <td>5.527</td>\n",
       "      <td>3.514</td>\n",
       "      <td>1.5990</td>\n",
       "      <td>5.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14.70</td>\n",
       "      <td>14.21</td>\n",
       "      <td>0.9153</td>\n",
       "      <td>5.205</td>\n",
       "      <td>3.466</td>\n",
       "      <td>1.7670</td>\n",
       "      <td>4.649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>12.72</td>\n",
       "      <td>13.57</td>\n",
       "      <td>0.8686</td>\n",
       "      <td>5.226</td>\n",
       "      <td>3.049</td>\n",
       "      <td>4.1020</td>\n",
       "      <td>4.914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14.16</td>\n",
       "      <td>14.40</td>\n",
       "      <td>0.8584</td>\n",
       "      <td>5.658</td>\n",
       "      <td>3.129</td>\n",
       "      <td>3.0720</td>\n",
       "      <td>5.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>14.11</td>\n",
       "      <td>14.26</td>\n",
       "      <td>0.8722</td>\n",
       "      <td>5.520</td>\n",
       "      <td>3.168</td>\n",
       "      <td>2.6880</td>\n",
       "      <td>5.219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.88</td>\n",
       "      <td>14.90</td>\n",
       "      <td>0.8988</td>\n",
       "      <td>5.618</td>\n",
       "      <td>3.507</td>\n",
       "      <td>0.7651</td>\n",
       "      <td>5.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>12.08</td>\n",
       "      <td>13.23</td>\n",
       "      <td>0.8664</td>\n",
       "      <td>5.099</td>\n",
       "      <td>2.936</td>\n",
       "      <td>1.4150</td>\n",
       "      <td>4.961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>15.01</td>\n",
       "      <td>14.76</td>\n",
       "      <td>0.8657</td>\n",
       "      <td>5.789</td>\n",
       "      <td>3.245</td>\n",
       "      <td>1.7910</td>\n",
       "      <td>5.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16.19</td>\n",
       "      <td>15.16</td>\n",
       "      <td>0.8849</td>\n",
       "      <td>5.833</td>\n",
       "      <td>3.421</td>\n",
       "      <td>0.9030</td>\n",
       "      <td>5.307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>13.02</td>\n",
       "      <td>13.76</td>\n",
       "      <td>0.8641</td>\n",
       "      <td>5.395</td>\n",
       "      <td>3.026</td>\n",
       "      <td>3.3730</td>\n",
       "      <td>4.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>12.74</td>\n",
       "      <td>13.67</td>\n",
       "      <td>0.8564</td>\n",
       "      <td>5.395</td>\n",
       "      <td>2.956</td>\n",
       "      <td>2.5040</td>\n",
       "      <td>4.869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>14.11</td>\n",
       "      <td>14.18</td>\n",
       "      <td>0.8820</td>\n",
       "      <td>5.541</td>\n",
       "      <td>3.221</td>\n",
       "      <td>2.7540</td>\n",
       "      <td>5.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>13.45</td>\n",
       "      <td>14.02</td>\n",
       "      <td>0.8604</td>\n",
       "      <td>5.516</td>\n",
       "      <td>3.065</td>\n",
       "      <td>3.5310</td>\n",
       "      <td>5.097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>11.41</td>\n",
       "      <td>12.95</td>\n",
       "      <td>0.8560</td>\n",
       "      <td>5.090</td>\n",
       "      <td>2.775</td>\n",
       "      <td>4.9570</td>\n",
       "      <td>4.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>12.46</td>\n",
       "      <td>13.41</td>\n",
       "      <td>0.8706</td>\n",
       "      <td>5.236</td>\n",
       "      <td>3.017</td>\n",
       "      <td>4.9870</td>\n",
       "      <td>5.147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>12.19</td>\n",
       "      <td>13.36</td>\n",
       "      <td>0.8579</td>\n",
       "      <td>5.240</td>\n",
       "      <td>2.909</td>\n",
       "      <td>4.8570</td>\n",
       "      <td>5.158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>11.65</td>\n",
       "      <td>13.07</td>\n",
       "      <td>0.8575</td>\n",
       "      <td>5.108</td>\n",
       "      <td>2.850</td>\n",
       "      <td>5.2090</td>\n",
       "      <td>5.135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>12.89</td>\n",
       "      <td>13.77</td>\n",
       "      <td>0.8541</td>\n",
       "      <td>5.495</td>\n",
       "      <td>3.026</td>\n",
       "      <td>6.1850</td>\n",
       "      <td>5.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>11.56</td>\n",
       "      <td>13.31</td>\n",
       "      <td>0.8198</td>\n",
       "      <td>5.363</td>\n",
       "      <td>2.683</td>\n",
       "      <td>4.0620</td>\n",
       "      <td>5.182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>11.81</td>\n",
       "      <td>13.45</td>\n",
       "      <td>0.8198</td>\n",
       "      <td>5.413</td>\n",
       "      <td>2.716</td>\n",
       "      <td>4.8980</td>\n",
       "      <td>5.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>10.91</td>\n",
       "      <td>12.80</td>\n",
       "      <td>0.8372</td>\n",
       "      <td>5.088</td>\n",
       "      <td>2.675</td>\n",
       "      <td>4.1790</td>\n",
       "      <td>4.956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>11.23</td>\n",
       "      <td>12.82</td>\n",
       "      <td>0.8594</td>\n",
       "      <td>5.089</td>\n",
       "      <td>2.821</td>\n",
       "      <td>7.5240</td>\n",
       "      <td>4.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>10.59</td>\n",
       "      <td>12.41</td>\n",
       "      <td>0.8648</td>\n",
       "      <td>4.899</td>\n",
       "      <td>2.787</td>\n",
       "      <td>4.9750</td>\n",
       "      <td>4.794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>10.93</td>\n",
       "      <td>12.80</td>\n",
       "      <td>0.8390</td>\n",
       "      <td>5.046</td>\n",
       "      <td>2.717</td>\n",
       "      <td>5.3980</td>\n",
       "      <td>5.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>11.27</td>\n",
       "      <td>12.86</td>\n",
       "      <td>0.8563</td>\n",
       "      <td>5.091</td>\n",
       "      <td>2.804</td>\n",
       "      <td>3.9850</td>\n",
       "      <td>5.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>11.87</td>\n",
       "      <td>13.02</td>\n",
       "      <td>0.8795</td>\n",
       "      <td>5.132</td>\n",
       "      <td>2.953</td>\n",
       "      <td>3.5970</td>\n",
       "      <td>5.132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>10.82</td>\n",
       "      <td>12.83</td>\n",
       "      <td>0.8256</td>\n",
       "      <td>5.180</td>\n",
       "      <td>2.630</td>\n",
       "      <td>4.8530</td>\n",
       "      <td>5.089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>12.11</td>\n",
       "      <td>13.27</td>\n",
       "      <td>0.8639</td>\n",
       "      <td>5.236</td>\n",
       "      <td>2.975</td>\n",
       "      <td>4.1320</td>\n",
       "      <td>5.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>12.80</td>\n",
       "      <td>13.47</td>\n",
       "      <td>0.8860</td>\n",
       "      <td>5.160</td>\n",
       "      <td>3.126</td>\n",
       "      <td>4.8730</td>\n",
       "      <td>4.914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>12.79</td>\n",
       "      <td>13.53</td>\n",
       "      <td>0.8786</td>\n",
       "      <td>5.224</td>\n",
       "      <td>3.054</td>\n",
       "      <td>5.4830</td>\n",
       "      <td>4.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>13.37</td>\n",
       "      <td>13.78</td>\n",
       "      <td>0.8849</td>\n",
       "      <td>5.320</td>\n",
       "      <td>3.128</td>\n",
       "      <td>4.6700</td>\n",
       "      <td>5.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>12.62</td>\n",
       "      <td>13.67</td>\n",
       "      <td>0.8481</td>\n",
       "      <td>5.410</td>\n",
       "      <td>2.911</td>\n",
       "      <td>3.3060</td>\n",
       "      <td>5.231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>12.76</td>\n",
       "      <td>13.38</td>\n",
       "      <td>0.8964</td>\n",
       "      <td>5.073</td>\n",
       "      <td>3.155</td>\n",
       "      <td>2.8280</td>\n",
       "      <td>4.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>12.38</td>\n",
       "      <td>13.44</td>\n",
       "      <td>0.8609</td>\n",
       "      <td>5.219</td>\n",
       "      <td>2.989</td>\n",
       "      <td>5.4720</td>\n",
       "      <td>5.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>12.67</td>\n",
       "      <td>13.32</td>\n",
       "      <td>0.8977</td>\n",
       "      <td>4.984</td>\n",
       "      <td>3.135</td>\n",
       "      <td>2.3000</td>\n",
       "      <td>4.745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>11.18</td>\n",
       "      <td>12.72</td>\n",
       "      <td>0.8680</td>\n",
       "      <td>5.009</td>\n",
       "      <td>2.810</td>\n",
       "      <td>4.0510</td>\n",
       "      <td>4.828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>12.70</td>\n",
       "      <td>13.41</td>\n",
       "      <td>0.8874</td>\n",
       "      <td>5.183</td>\n",
       "      <td>3.091</td>\n",
       "      <td>8.4560</td>\n",
       "      <td>5.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>12.37</td>\n",
       "      <td>13.47</td>\n",
       "      <td>0.8567</td>\n",
       "      <td>5.204</td>\n",
       "      <td>2.960</td>\n",
       "      <td>3.9190</td>\n",
       "      <td>5.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>12.19</td>\n",
       "      <td>13.20</td>\n",
       "      <td>0.8783</td>\n",
       "      <td>5.137</td>\n",
       "      <td>2.981</td>\n",
       "      <td>3.6310</td>\n",
       "      <td>4.870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>11.23</td>\n",
       "      <td>12.88</td>\n",
       "      <td>0.8511</td>\n",
       "      <td>5.140</td>\n",
       "      <td>2.795</td>\n",
       "      <td>4.3250</td>\n",
       "      <td>5.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>13.20</td>\n",
       "      <td>13.66</td>\n",
       "      <td>0.8883</td>\n",
       "      <td>5.236</td>\n",
       "      <td>3.232</td>\n",
       "      <td>8.3150</td>\n",
       "      <td>5.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>11.84</td>\n",
       "      <td>13.21</td>\n",
       "      <td>0.8521</td>\n",
       "      <td>5.175</td>\n",
       "      <td>2.836</td>\n",
       "      <td>3.5980</td>\n",
       "      <td>5.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>12.30</td>\n",
       "      <td>13.34</td>\n",
       "      <td>0.8684</td>\n",
       "      <td>5.243</td>\n",
       "      <td>2.974</td>\n",
       "      <td>5.6370</td>\n",
       "      <td>5.063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        V1     V2      V3     V4     V5      V6     V7\n",
       "0    15.26  14.84  0.8710  5.763  3.312  2.2210  5.220\n",
       "1    14.88  14.57  0.8811  5.554  3.333  1.0180  4.956\n",
       "2    14.29  14.09  0.9050  5.291  3.337  2.6990  4.825\n",
       "3    13.84  13.94  0.8955  5.324  3.379  2.2590  4.805\n",
       "4    16.14  14.99  0.9034  5.658  3.562  1.3550  5.175\n",
       "5    14.38  14.21  0.8951  5.386  3.312  2.4620  4.956\n",
       "6    14.69  14.49  0.8799  5.563  3.259  3.5860  5.219\n",
       "7    14.11  14.10  0.8911  5.420  3.302  2.7000  5.000\n",
       "8    16.63  15.46  0.8747  6.053  3.465  2.0400  5.877\n",
       "9    16.44  15.25  0.8880  5.884  3.505  1.9690  5.533\n",
       "10   15.26  14.85  0.8696  5.714  3.242  4.5430  5.314\n",
       "11   14.03  14.16  0.8796  5.438  3.201  1.7170  5.001\n",
       "12   13.89  14.02  0.8880  5.439  3.199  3.9860  4.738\n",
       "13   13.78  14.06  0.8759  5.479  3.156  3.1360  4.872\n",
       "14   13.74  14.05  0.8744  5.482  3.114  2.9320  4.825\n",
       "15   14.59  14.28  0.8993  5.351  3.333  4.1850  4.781\n",
       "16   13.99  13.83  0.9183  5.119  3.383  5.2340  4.781\n",
       "17   15.69  14.75  0.9058  5.527  3.514  1.5990  5.046\n",
       "18   14.70  14.21  0.9153  5.205  3.466  1.7670  4.649\n",
       "19   12.72  13.57  0.8686  5.226  3.049  4.1020  4.914\n",
       "20   14.16  14.40  0.8584  5.658  3.129  3.0720  5.176\n",
       "21   14.11  14.26  0.8722  5.520  3.168  2.6880  5.219\n",
       "22   15.88  14.90  0.8988  5.618  3.507  0.7651  5.091\n",
       "23   12.08  13.23  0.8664  5.099  2.936  1.4150  4.961\n",
       "24   15.01  14.76  0.8657  5.789  3.245  1.7910  5.001\n",
       "25   16.19  15.16  0.8849  5.833  3.421  0.9030  5.307\n",
       "26   13.02  13.76  0.8641  5.395  3.026  3.3730  4.825\n",
       "27   12.74  13.67  0.8564  5.395  2.956  2.5040  4.869\n",
       "28   14.11  14.18  0.8820  5.541  3.221  2.7540  5.038\n",
       "29   13.45  14.02  0.8604  5.516  3.065  3.5310  5.097\n",
       "..     ...    ...     ...    ...    ...     ...    ...\n",
       "180  11.41  12.95  0.8560  5.090  2.775  4.9570  4.825\n",
       "181  12.46  13.41  0.8706  5.236  3.017  4.9870  5.147\n",
       "182  12.19  13.36  0.8579  5.240  2.909  4.8570  5.158\n",
       "183  11.65  13.07  0.8575  5.108  2.850  5.2090  5.135\n",
       "184  12.89  13.77  0.8541  5.495  3.026  6.1850  5.316\n",
       "185  11.56  13.31  0.8198  5.363  2.683  4.0620  5.182\n",
       "186  11.81  13.45  0.8198  5.413  2.716  4.8980  5.352\n",
       "187  10.91  12.80  0.8372  5.088  2.675  4.1790  4.956\n",
       "188  11.23  12.82  0.8594  5.089  2.821  7.5240  4.957\n",
       "189  10.59  12.41  0.8648  4.899  2.787  4.9750  4.794\n",
       "190  10.93  12.80  0.8390  5.046  2.717  5.3980  5.045\n",
       "191  11.27  12.86  0.8563  5.091  2.804  3.9850  5.001\n",
       "192  11.87  13.02  0.8795  5.132  2.953  3.5970  5.132\n",
       "193  10.82  12.83  0.8256  5.180  2.630  4.8530  5.089\n",
       "194  12.11  13.27  0.8639  5.236  2.975  4.1320  5.012\n",
       "195  12.80  13.47  0.8860  5.160  3.126  4.8730  4.914\n",
       "196  12.79  13.53  0.8786  5.224  3.054  5.4830  4.958\n",
       "197  13.37  13.78  0.8849  5.320  3.128  4.6700  5.091\n",
       "198  12.62  13.67  0.8481  5.410  2.911  3.3060  5.231\n",
       "199  12.76  13.38  0.8964  5.073  3.155  2.8280  4.830\n",
       "200  12.38  13.44  0.8609  5.219  2.989  5.4720  5.045\n",
       "201  12.67  13.32  0.8977  4.984  3.135  2.3000  4.745\n",
       "202  11.18  12.72  0.8680  5.009  2.810  4.0510  4.828\n",
       "203  12.70  13.41  0.8874  5.183  3.091  8.4560  5.000\n",
       "204  12.37  13.47  0.8567  5.204  2.960  3.9190  5.001\n",
       "205  12.19  13.20  0.8783  5.137  2.981  3.6310  4.870\n",
       "206  11.23  12.88  0.8511  5.140  2.795  4.3250  5.003\n",
       "207  13.20  13.66  0.8883  5.236  3.232  8.3150  5.056\n",
       "208  11.84  13.21  0.8521  5.175  2.836  3.5980  5.044\n",
       "209  12.30  13.34  0.8684  5.243  2.974  5.6370  5.063\n",
       "\n",
       "[210 rows x 7 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>15.38</td>\n",
       "      <td>14.90</td>\n",
       "      <td>0.8706</td>\n",
       "      <td>5.884</td>\n",
       "      <td>3.268</td>\n",
       "      <td>4.4620</td>\n",
       "      <td>5.795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16.44</td>\n",
       "      <td>15.25</td>\n",
       "      <td>0.8880</td>\n",
       "      <td>5.884</td>\n",
       "      <td>3.505</td>\n",
       "      <td>1.9690</td>\n",
       "      <td>5.533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>19.31</td>\n",
       "      <td>16.59</td>\n",
       "      <td>0.8815</td>\n",
       "      <td>6.341</td>\n",
       "      <td>3.810</td>\n",
       "      <td>3.4770</td>\n",
       "      <td>6.238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>11.40</td>\n",
       "      <td>13.08</td>\n",
       "      <td>0.8375</td>\n",
       "      <td>5.136</td>\n",
       "      <td>2.763</td>\n",
       "      <td>5.5880</td>\n",
       "      <td>5.089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>21.18</td>\n",
       "      <td>17.21</td>\n",
       "      <td>0.8989</td>\n",
       "      <td>6.573</td>\n",
       "      <td>4.033</td>\n",
       "      <td>5.7800</td>\n",
       "      <td>6.231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>20.20</td>\n",
       "      <td>16.89</td>\n",
       "      <td>0.8894</td>\n",
       "      <td>6.285</td>\n",
       "      <td>3.864</td>\n",
       "      <td>5.1730</td>\n",
       "      <td>6.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>18.88</td>\n",
       "      <td>16.26</td>\n",
       "      <td>0.8969</td>\n",
       "      <td>6.084</td>\n",
       "      <td>3.764</td>\n",
       "      <td>1.6490</td>\n",
       "      <td>6.109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>12.10</td>\n",
       "      <td>13.15</td>\n",
       "      <td>0.8793</td>\n",
       "      <td>5.105</td>\n",
       "      <td>2.941</td>\n",
       "      <td>2.2010</td>\n",
       "      <td>5.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>19.13</td>\n",
       "      <td>16.31</td>\n",
       "      <td>0.9035</td>\n",
       "      <td>6.183</td>\n",
       "      <td>3.902</td>\n",
       "      <td>2.1090</td>\n",
       "      <td>5.924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>18.14</td>\n",
       "      <td>16.12</td>\n",
       "      <td>0.8772</td>\n",
       "      <td>6.059</td>\n",
       "      <td>3.563</td>\n",
       "      <td>3.6190</td>\n",
       "      <td>6.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>20.03</td>\n",
       "      <td>16.90</td>\n",
       "      <td>0.8811</td>\n",
       "      <td>6.493</td>\n",
       "      <td>3.857</td>\n",
       "      <td>3.0630</td>\n",
       "      <td>6.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>12.79</td>\n",
       "      <td>13.53</td>\n",
       "      <td>0.8786</td>\n",
       "      <td>5.224</td>\n",
       "      <td>3.054</td>\n",
       "      <td>5.4830</td>\n",
       "      <td>4.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>13.37</td>\n",
       "      <td>13.78</td>\n",
       "      <td>0.8849</td>\n",
       "      <td>5.320</td>\n",
       "      <td>3.128</td>\n",
       "      <td>4.6700</td>\n",
       "      <td>5.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>12.74</td>\n",
       "      <td>13.67</td>\n",
       "      <td>0.8564</td>\n",
       "      <td>5.395</td>\n",
       "      <td>2.956</td>\n",
       "      <td>2.5040</td>\n",
       "      <td>4.869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>17.32</td>\n",
       "      <td>15.91</td>\n",
       "      <td>0.8599</td>\n",
       "      <td>6.064</td>\n",
       "      <td>3.403</td>\n",
       "      <td>3.8240</td>\n",
       "      <td>5.922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>14.92</td>\n",
       "      <td>14.43</td>\n",
       "      <td>0.9006</td>\n",
       "      <td>5.384</td>\n",
       "      <td>3.412</td>\n",
       "      <td>1.1420</td>\n",
       "      <td>5.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>10.59</td>\n",
       "      <td>12.41</td>\n",
       "      <td>0.8648</td>\n",
       "      <td>4.899</td>\n",
       "      <td>2.787</td>\n",
       "      <td>4.9750</td>\n",
       "      <td>4.794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>11.82</td>\n",
       "      <td>13.40</td>\n",
       "      <td>0.8274</td>\n",
       "      <td>5.314</td>\n",
       "      <td>2.777</td>\n",
       "      <td>4.4710</td>\n",
       "      <td>5.178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>14.52</td>\n",
       "      <td>14.60</td>\n",
       "      <td>0.8557</td>\n",
       "      <td>5.741</td>\n",
       "      <td>3.113</td>\n",
       "      <td>1.4810</td>\n",
       "      <td>5.487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>12.80</td>\n",
       "      <td>13.47</td>\n",
       "      <td>0.8860</td>\n",
       "      <td>5.160</td>\n",
       "      <td>3.126</td>\n",
       "      <td>4.8730</td>\n",
       "      <td>4.914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>18.98</td>\n",
       "      <td>16.57</td>\n",
       "      <td>0.8687</td>\n",
       "      <td>6.449</td>\n",
       "      <td>3.552</td>\n",
       "      <td>2.1440</td>\n",
       "      <td>6.453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>18.94</td>\n",
       "      <td>16.32</td>\n",
       "      <td>0.8942</td>\n",
       "      <td>6.144</td>\n",
       "      <td>3.825</td>\n",
       "      <td>2.9080</td>\n",
       "      <td>5.949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>12.02</td>\n",
       "      <td>13.33</td>\n",
       "      <td>0.8503</td>\n",
       "      <td>5.350</td>\n",
       "      <td>2.810</td>\n",
       "      <td>4.2710</td>\n",
       "      <td>5.308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>12.08</td>\n",
       "      <td>13.23</td>\n",
       "      <td>0.8664</td>\n",
       "      <td>5.099</td>\n",
       "      <td>2.936</td>\n",
       "      <td>1.4150</td>\n",
       "      <td>4.961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>14.46</td>\n",
       "      <td>14.35</td>\n",
       "      <td>0.8818</td>\n",
       "      <td>5.388</td>\n",
       "      <td>3.377</td>\n",
       "      <td>2.8020</td>\n",
       "      <td>5.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>11.14</td>\n",
       "      <td>12.79</td>\n",
       "      <td>0.8558</td>\n",
       "      <td>5.011</td>\n",
       "      <td>2.794</td>\n",
       "      <td>6.3880</td>\n",
       "      <td>5.049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>11.42</td>\n",
       "      <td>12.86</td>\n",
       "      <td>0.8683</td>\n",
       "      <td>5.008</td>\n",
       "      <td>2.850</td>\n",
       "      <td>2.7000</td>\n",
       "      <td>4.607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>11.81</td>\n",
       "      <td>13.45</td>\n",
       "      <td>0.8198</td>\n",
       "      <td>5.413</td>\n",
       "      <td>2.716</td>\n",
       "      <td>4.8980</td>\n",
       "      <td>5.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>16.84</td>\n",
       "      <td>15.67</td>\n",
       "      <td>0.8623</td>\n",
       "      <td>5.998</td>\n",
       "      <td>3.484</td>\n",
       "      <td>4.6750</td>\n",
       "      <td>5.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>12.70</td>\n",
       "      <td>13.71</td>\n",
       "      <td>0.8491</td>\n",
       "      <td>5.386</td>\n",
       "      <td>2.911</td>\n",
       "      <td>3.2600</td>\n",
       "      <td>5.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>11.83</td>\n",
       "      <td>13.23</td>\n",
       "      <td>0.8496</td>\n",
       "      <td>5.263</td>\n",
       "      <td>2.840</td>\n",
       "      <td>5.1950</td>\n",
       "      <td>5.307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>14.28</td>\n",
       "      <td>14.17</td>\n",
       "      <td>0.8944</td>\n",
       "      <td>5.397</td>\n",
       "      <td>3.298</td>\n",
       "      <td>6.6850</td>\n",
       "      <td>5.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>12.15</td>\n",
       "      <td>13.45</td>\n",
       "      <td>0.8443</td>\n",
       "      <td>5.417</td>\n",
       "      <td>2.837</td>\n",
       "      <td>3.6380</td>\n",
       "      <td>5.338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>18.27</td>\n",
       "      <td>16.09</td>\n",
       "      <td>0.8870</td>\n",
       "      <td>6.173</td>\n",
       "      <td>3.651</td>\n",
       "      <td>2.4430</td>\n",
       "      <td>6.197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>11.84</td>\n",
       "      <td>13.21</td>\n",
       "      <td>0.8521</td>\n",
       "      <td>5.175</td>\n",
       "      <td>2.836</td>\n",
       "      <td>3.5980</td>\n",
       "      <td>5.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>19.38</td>\n",
       "      <td>16.72</td>\n",
       "      <td>0.8716</td>\n",
       "      <td>6.303</td>\n",
       "      <td>3.791</td>\n",
       "      <td>3.6780</td>\n",
       "      <td>5.965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>14.34</td>\n",
       "      <td>14.37</td>\n",
       "      <td>0.8726</td>\n",
       "      <td>5.630</td>\n",
       "      <td>3.190</td>\n",
       "      <td>1.3130</td>\n",
       "      <td>5.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>19.15</td>\n",
       "      <td>16.45</td>\n",
       "      <td>0.8890</td>\n",
       "      <td>6.245</td>\n",
       "      <td>3.815</td>\n",
       "      <td>3.0840</td>\n",
       "      <td>6.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>15.11</td>\n",
       "      <td>14.54</td>\n",
       "      <td>0.8986</td>\n",
       "      <td>5.579</td>\n",
       "      <td>3.462</td>\n",
       "      <td>3.1280</td>\n",
       "      <td>5.180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>13.07</td>\n",
       "      <td>13.92</td>\n",
       "      <td>0.8480</td>\n",
       "      <td>5.472</td>\n",
       "      <td>2.994</td>\n",
       "      <td>5.3040</td>\n",
       "      <td>5.395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>17.99</td>\n",
       "      <td>15.86</td>\n",
       "      <td>0.8992</td>\n",
       "      <td>5.890</td>\n",
       "      <td>3.694</td>\n",
       "      <td>2.0680</td>\n",
       "      <td>5.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>12.73</td>\n",
       "      <td>13.75</td>\n",
       "      <td>0.8458</td>\n",
       "      <td>5.412</td>\n",
       "      <td>2.882</td>\n",
       "      <td>3.5330</td>\n",
       "      <td>5.067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.29</td>\n",
       "      <td>14.09</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>5.291</td>\n",
       "      <td>3.337</td>\n",
       "      <td>2.6990</td>\n",
       "      <td>4.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>19.51</td>\n",
       "      <td>16.71</td>\n",
       "      <td>0.8780</td>\n",
       "      <td>6.366</td>\n",
       "      <td>3.801</td>\n",
       "      <td>2.9620</td>\n",
       "      <td>6.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>15.69</td>\n",
       "      <td>14.75</td>\n",
       "      <td>0.9058</td>\n",
       "      <td>5.527</td>\n",
       "      <td>3.514</td>\n",
       "      <td>1.5990</td>\n",
       "      <td>5.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>11.43</td>\n",
       "      <td>13.13</td>\n",
       "      <td>0.8335</td>\n",
       "      <td>5.176</td>\n",
       "      <td>2.719</td>\n",
       "      <td>2.2210</td>\n",
       "      <td>5.132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>13.94</td>\n",
       "      <td>14.17</td>\n",
       "      <td>0.8728</td>\n",
       "      <td>5.585</td>\n",
       "      <td>3.150</td>\n",
       "      <td>2.1240</td>\n",
       "      <td>5.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>13.45</td>\n",
       "      <td>14.02</td>\n",
       "      <td>0.8604</td>\n",
       "      <td>5.516</td>\n",
       "      <td>3.065</td>\n",
       "      <td>3.5310</td>\n",
       "      <td>5.097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>13.16</td>\n",
       "      <td>13.82</td>\n",
       "      <td>0.8662</td>\n",
       "      <td>5.454</td>\n",
       "      <td>2.975</td>\n",
       "      <td>0.8551</td>\n",
       "      <td>5.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>20.97</td>\n",
       "      <td>17.25</td>\n",
       "      <td>0.8859</td>\n",
       "      <td>6.563</td>\n",
       "      <td>3.991</td>\n",
       "      <td>4.6770</td>\n",
       "      <td>6.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>16.12</td>\n",
       "      <td>15.00</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>5.709</td>\n",
       "      <td>3.485</td>\n",
       "      <td>2.2700</td>\n",
       "      <td>5.443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>19.18</td>\n",
       "      <td>16.63</td>\n",
       "      <td>0.8717</td>\n",
       "      <td>6.369</td>\n",
       "      <td>3.681</td>\n",
       "      <td>3.3570</td>\n",
       "      <td>6.229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>14.01</td>\n",
       "      <td>14.29</td>\n",
       "      <td>0.8625</td>\n",
       "      <td>5.609</td>\n",
       "      <td>3.158</td>\n",
       "      <td>2.2170</td>\n",
       "      <td>5.132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>16.19</td>\n",
       "      <td>15.16</td>\n",
       "      <td>0.8849</td>\n",
       "      <td>5.833</td>\n",
       "      <td>3.421</td>\n",
       "      <td>0.9030</td>\n",
       "      <td>5.307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>11.26</td>\n",
       "      <td>13.01</td>\n",
       "      <td>0.8355</td>\n",
       "      <td>5.186</td>\n",
       "      <td>2.710</td>\n",
       "      <td>5.3350</td>\n",
       "      <td>5.092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>16.20</td>\n",
       "      <td>15.27</td>\n",
       "      <td>0.8734</td>\n",
       "      <td>5.826</td>\n",
       "      <td>3.464</td>\n",
       "      <td>2.8230</td>\n",
       "      <td>5.527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>18.96</td>\n",
       "      <td>16.20</td>\n",
       "      <td>0.9077</td>\n",
       "      <td>6.051</td>\n",
       "      <td>3.897</td>\n",
       "      <td>4.3340</td>\n",
       "      <td>5.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>11.35</td>\n",
       "      <td>13.12</td>\n",
       "      <td>0.8291</td>\n",
       "      <td>5.176</td>\n",
       "      <td>2.668</td>\n",
       "      <td>4.3370</td>\n",
       "      <td>5.132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>18.76</td>\n",
       "      <td>16.20</td>\n",
       "      <td>0.8984</td>\n",
       "      <td>6.172</td>\n",
       "      <td>3.796</td>\n",
       "      <td>3.1200</td>\n",
       "      <td>6.053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>20.24</td>\n",
       "      <td>16.91</td>\n",
       "      <td>0.8897</td>\n",
       "      <td>6.315</td>\n",
       "      <td>3.962</td>\n",
       "      <td>5.9010</td>\n",
       "      <td>6.188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        V1     V2      V3     V4     V5      V6     V7\n",
       "132  15.38  14.90  0.8706  5.884  3.268  4.4620  5.795\n",
       "9    16.44  15.25  0.8880  5.884  3.505  1.9690  5.533\n",
       "96   19.31  16.59  0.8815  6.341  3.810  3.4770  6.238\n",
       "173  11.40  13.08  0.8375  5.136  2.763  5.5880  5.089\n",
       "88   21.18  17.21  0.8989  6.573  4.033  5.7800  6.231\n",
       "82   20.20  16.89  0.8894  6.285  3.864  5.1730  6.187\n",
       "86   18.88  16.26  0.8969  6.084  3.764  1.6490  6.109\n",
       "165  12.10  13.15  0.8793  5.105  2.941  2.2010  5.056\n",
       "112  19.13  16.31  0.9035  6.183  3.902  2.1090  5.924\n",
       "121  18.14  16.12  0.8772  6.059  3.563  3.6190  6.011\n",
       "119  20.03  16.90  0.8811  6.493  3.857  3.0630  6.320\n",
       "196  12.79  13.53  0.8786  5.224  3.054  5.4830  4.958\n",
       "197  13.37  13.78  0.8849  5.320  3.128  4.6700  5.091\n",
       "27   12.74  13.67  0.8564  5.395  2.956  2.5040  4.869\n",
       "76   17.32  15.91  0.8599  6.064  3.403  3.8240  5.922\n",
       "57   14.92  14.43  0.9006  5.384  3.412  1.1420  5.088\n",
       "189  10.59  12.41  0.8648  4.899  2.787  4.9750  4.794\n",
       "144  11.82  13.40  0.8274  5.314  2.777  4.4710  5.178\n",
       "54   14.52  14.60  0.8557  5.741  3.113  1.4810  5.487\n",
       "195  12.80  13.47  0.8860  5.160  3.126  4.8730  4.914\n",
       "97   18.98  16.57  0.8687  6.449  3.552  2.1440  6.453\n",
       "131  18.94  16.32  0.8942  6.144  3.825  2.9080  5.949\n",
       "161  12.02  13.33  0.8503  5.350  2.810  4.2710  5.308\n",
       "23   12.08  13.23  0.8664  5.099  2.936  1.4150  4.961\n",
       "56   14.46  14.35  0.8818  5.388  3.377  2.8020  5.044\n",
       "164  11.14  12.79  0.8558  5.011  2.794  6.3880  5.049\n",
       "60   11.42  12.86  0.8683  5.008  2.850  2.7000  4.607\n",
       "186  11.81  13.45  0.8198  5.413  2.716  4.8980  5.352\n",
       "71   16.84  15.67  0.8623  5.998  3.484  4.6750  5.877\n",
       "148  12.70  13.71  0.8491  5.386  2.911  3.2600  5.316\n",
       "..     ...    ...     ...    ...    ...     ...    ...\n",
       "150  11.83  13.23  0.8496  5.263  2.840  5.1950  5.307\n",
       "39   14.28  14.17  0.8944  5.397  3.298  6.6850  5.001\n",
       "167  12.15  13.45  0.8443  5.417  2.837  3.6380  5.338\n",
       "85   18.27  16.09  0.8870  6.173  3.651  2.4430  6.197\n",
       "208  11.84  13.21  0.8521  5.175  2.836  3.5980  5.044\n",
       "111  19.38  16.72  0.8716  6.303  3.791  3.6780  5.965\n",
       "66   14.34  14.37  0.8726  5.630  3.190  1.3130  5.150\n",
       "117  19.15  16.45  0.8890  6.245  3.815  3.0840  6.185\n",
       "44   15.11  14.54  0.8986  5.579  3.462  3.1280  5.180\n",
       "140  13.07  13.92  0.8480  5.472  2.994  5.3040  5.395\n",
       "101  17.99  15.86  0.8992  5.890  3.694  2.0680  5.837\n",
       "69   12.73  13.75  0.8458  5.412  2.882  3.5330  5.067\n",
       "2    14.29  14.09  0.9050  5.291  3.337  2.6990  4.825\n",
       "84   19.51  16.71  0.8780  6.366  3.801  2.9620  6.185\n",
       "17   15.69  14.75  0.9058  5.527  3.514  1.5990  5.046\n",
       "146  11.43  13.13  0.8335  5.176  2.719  2.2210  5.132\n",
       "33   13.94  14.17  0.8728  5.585  3.150  2.1240  5.012\n",
       "29   13.45  14.02  0.8604  5.516  3.065  3.5310  5.097\n",
       "30   13.16  13.82  0.8662  5.454  2.975  0.8551  5.056\n",
       "114  20.97  17.25  0.8859  6.563  3.991  4.6770  6.316\n",
       "35   16.12  15.00  0.9000  5.709  3.485  2.2700  5.443\n",
       "103  19.18  16.63  0.8717  6.369  3.681  3.3570  6.229\n",
       "67   14.01  14.29  0.8625  5.609  3.158  2.2170  5.132\n",
       "25   16.19  15.16  0.8849  5.833  3.421  0.9030  5.307\n",
       "176  11.26  13.01  0.8355  5.186  2.710  5.3350  5.092\n",
       "36   16.20  15.27  0.8734  5.826  3.464  2.8230  5.527\n",
       "116  18.96  16.20  0.9077  6.051  3.897  4.3340  5.750\n",
       "168  11.35  13.12  0.8291  5.176  2.668  4.3370  5.132\n",
       "91   18.76  16.20  0.8984  6.172  3.796  3.1200  6.053\n",
       "120  20.24  16.91  0.8897  6.315  3.962  5.9010  6.188\n",
       "\n",
       "[157 rows x 7 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 0, 1, 0, 2, 2, 0, 2, 1, 1, 2,\n",
       "       0, 0, 2, 0, 2, 1, 2, 0, 1, 1, 1, 2, 0, 2, 0, 2, 0, 1, 2, 2, 2, 1, 2,\n",
       "       2, 1, 1, 0, 2, 0, 1, 2, 1, 0, 2, 0, 2, 0, 2, 2, 0, 2, 2, 0, 2, 2, 2,\n",
       "       1, 2, 2, 0, 1, 0, 1, 0, 2, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 2, 0, 0, 2,\n",
       "       0, 2, 2, 1, 0, 0, 2, 1, 2, 1, 1, 2, 2, 1, 0, 2, 0, 1, 1, 0, 0, 1, 1,\n",
       "       0, 1, 1, 2, 1, 0, 1, 0, 1, 0, 2, 2, 2, 0, 2, 1, 2, 1, 0, 1, 0, 2, 1,\n",
       "       0, 0, 1, 0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 1, 2, 1, 1])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We apply the Logistic regression of sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv=None, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1.0, max_iter=100,\n",
       "           multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "           refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegressionCV()\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.96      0.98      0.97        56\n",
      "          2       1.00      0.98      0.99        51\n",
      "          3       0.98      0.98      0.98        50\n",
      "\n",
      "avg / total       0.98      0.98      0.98       157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(clf.predict(X_train),y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.92      0.86      0.89        14\n",
      "          2       0.95      1.00      0.97        19\n",
      "          3       0.95      0.95      0.95        20\n",
      "\n",
      "avg / total       0.94      0.94      0.94        53\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(clf.predict(X_test),y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we implement a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 15.38  ,  14.9   ,   0.8706, ...,   3.268 ,   4.462 ,   5.795 ],\n",
       "       [ 16.44  ,  15.25  ,   0.888 , ...,   3.505 ,   1.969 ,   5.533 ],\n",
       "       [ 19.31  ,  16.59  ,   0.8815, ...,   3.81  ,   3.477 ,   6.238 ],\n",
       "       ..., \n",
       "       [ 11.35  ,  13.12  ,   0.8291, ...,   2.668 ,   4.337 ,   5.132 ],\n",
       "       [ 18.76  ,  16.2   ,   0.8984, ...,   3.796 ,   3.12  ,   6.053 ],\n",
       "       [ 20.24  ,  16.91  ,   0.8897, ...,   3.962 ,   5.901 ,   6.188 ]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t = np.array(X_train)\n",
    "X_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157, 7)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157,)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_t = np.array(y_train)\n",
    "y_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 0, 1, 0, 2, 2, 0, 2, 1, 1, 2,\n",
       "       0, 0, 2, 0, 2, 1, 2, 0, 1, 1, 1, 2, 0, 2, 0, 2, 0, 1, 2, 2, 2, 1, 2,\n",
       "       2, 1, 1, 0, 2, 0, 1, 2, 1, 0, 2, 0, 2, 0, 2, 2, 0, 2, 2, 0, 2, 2, 2,\n",
       "       1, 2, 2, 0, 1, 0, 1, 0, 2, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 2, 0, 0, 2,\n",
       "       0, 2, 2, 1, 0, 0, 2, 1, 2, 1, 1, 2, 2, 1, 0, 2, 0, 1, 1, 0, 0, 1, 1,\n",
       "       0, 1, 1, 2, 1, 0, 1, 0, 1, 0, 2, 2, 2, 0, 2, 1, 2, 1, 0, 1, 0, 2, 1,\n",
       "       0, 0, 1, 0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 1, 2, 1, 1])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53,)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ts = np.array(X_test)\n",
    "y_ts = np.array(y_test)\n",
    "y_ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 2, 2, 2, 2, 2, 1, 0, 2, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       0, 0, 0, 1, 2, 0, 0, 2, 0, 1, 1, 1, 2, 1, 0, 1, 2, 0, 1, 2, 2, 1, 0,\n",
       "       0, 0, 1, 2, 2, 2, 0])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_ = y_t.reshape(len(y_t), 1)\n",
    "y_ = onehot_encoder.fit_transform(y_)\n",
    "y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 0, 1, 0, 2, 2, 0, 2, 1, 1, 2,\n",
       "       0, 0, 2, 0, 2, 1, 2, 0, 1, 1, 1, 2, 0, 2, 0, 2, 0, 1, 2, 2, 2, 1, 2,\n",
       "       2, 1, 1, 0, 2, 0, 1, 2, 1, 0, 2, 0, 2, 0, 2, 2, 0, 2, 2, 0, 2, 2, 2,\n",
       "       1, 2, 2, 0, 1, 0, 1, 0, 2, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 2, 0, 0, 2,\n",
       "       0, 2, 2, 1, 0, 0, 2, 1, 2, 1, 1, 2, 2, 1, 0, 2, 0, 1, 1, 0, 0, 1, 1,\n",
       "       0, 1, 1, 2, 1, 0, 1, 0, 1, 0, 2, 2, 2, 0, 2, 1, 2, 1, 0, 1, 0, 2, 1,\n",
       "       0, 0, 1, 0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 1, 2, 1, 1])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parameters\n",
    "episilon = 0.01\n",
    "beta1 = 0.9\n",
    "reg_lambda = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to train the neural net\n",
    "# It will be a 3 layer NN with 7 input neurons,one hidden layer and 3 output neurons\n",
    "def build_NN(nn_hid,epochs = 2000,episilon = 0.01,beta1 = 0.9,reg_lambda = 0.01,print_loss = False,print_agr = False,norm = False):\n",
    "    model = {}\n",
    "    effective_loss = []\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(7,nn_hid) / np.sqrt(7)\n",
    "    b1 = np.zeros((1,nn_hid))\n",
    "    W2 = np.random.randn(nn_hid,3) / np.sqrt(nn_hid)\n",
    "    b2 = np.zeros((1,3))\n",
    "    Vw1 = np.zeros((7, nn_hid))\n",
    "    Vw2 = np.zeros((nn_hid, 3))\n",
    "    Vb1 = np.zeros((1, nn_hid))\n",
    "    Vb2 = np.zeros((1, 3))\n",
    "    num_example = 1\n",
    "    if norm == True:\n",
    "        num_example = len(y_t)\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    y_ = y_t.reshape(len(y_t), 1)\n",
    "    y_ = onehot_encoder.fit_transform(y_)\n",
    "    beta1 = 0.9\n",
    "    for k in range(0,epochs):\n",
    "        z1 = X_t.dot(W1) + b1\n",
    "        a1 = np.tanh(z1)\n",
    "        z2 = a1.dot(W2) + b2\n",
    "        a2 = np.exp(z2)\n",
    "        prob = a2 / np.sum(a2,axis=1,keepdims=True)\n",
    "        #prob = np.argmax(prob,axis=1)\n",
    "        # Backpropagation\n",
    "        dz2 = prob - y_\n",
    "        dW2 = (a1.T).dot(dz2)\n",
    "        db2 = np.sum(dz2,axis=0,keepdims=True)\n",
    "        da1 = dz2.dot(W2.T)\n",
    "        dz1 = da1*(1-np.power(a1,2))\n",
    "        dW1 = (X_t.T).dot(dz1)\n",
    "        db1 = np.sum(dz1,axis=0)\n",
    "        \n",
    "        # Adding the regulization\n",
    "        dW2 += reg_lambda*W2\n",
    "        dW1 += reg_lambda*W1\n",
    "        \n",
    "        # Apply the momentum\n",
    "        Vw1 = beta1*Vw1 + (1-beta1)*dW1\n",
    "        Vw2 = beta1*Vw2 + (1-beta1)*dW2\n",
    "        Vb1 = beta1*Vb1 + (1-beta1)*db1\n",
    "        Vb2 = beta1*Vb2 + (1-beta1)*db2\n",
    "        # gradient descent\n",
    "        W1 += -(1./num_example) *episilon*Vw1\n",
    "        b1 += -(1./num_example)*episilon*db1\n",
    "        W2 += -(1./num_example)*episilon*Vw2\n",
    "        b2 += -(1./num_example)*episilon*Vb2\n",
    "        \n",
    "        # Assigning the update parameters to the model\n",
    "        model = {'W1':W1,'b1':b1,'W2':W2,'b2':b2}\n",
    "        effective_loss.append(cal_loss(model,X_t,y_))\n",
    "        if print_agr and k % 1000 == 0:\n",
    "            print('The parameters are : W1',W1,'\\nW2',W2,'\\nb1',b1,'\\nb2',b2)\n",
    "        if print_loss and k % 1000 == 0:\n",
    "            print('The loss at ',k,' instance is :',cal_loss(model,X_t,y_))\n",
    "    return model,np.array(effective_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to train the neural net\n",
    "# It will be a 3 layer NN with 7 input neurons,one hidden layer and 3 output neurons\n",
    "def build_NN_in_batch(batch_size = 8,nn_hid = 3,epochs = 2000,episilon = 0.01,beta1 = 0.9,reg_lambda = 0.01,print_loss = False,print_agr = False,norm = False):\n",
    "    model = {}\n",
    "    effective_loss = []\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(7,nn_hid) / np.sqrt(7)\n",
    "    b1 = np.zeros((1,nn_hid))\n",
    "    W2 = np.random.randn(nn_hid,3) / np.sqrt(nn_hid)\n",
    "    b2 = np.zeros((1,3))\n",
    "    num_example = 1\n",
    "    i = 0\n",
    "    t = batch_size\n",
    "    if norm == True:\n",
    "        num_example = len(y_t)\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    y_ = y_t.reshape(len(y_t), 1)\n",
    "    y_ = onehot_encoder.fit_transform(y_)\n",
    "    for k in range(0,epochs):\n",
    "        z1 = X_t[i:t,:].dot(W1) + b1\n",
    "        a1 = np.tanh(z1)\n",
    "        z2 = a1.dot(W2) + b2\n",
    "        a2 = np.exp(z2)\n",
    "        prob = a2 / np.sum(a2,axis=1,keepdims=True)\n",
    "        #prob = np.argmax(prob,axis=1)\n",
    "        # Backpropagation\n",
    "        dz2 = prob - y_[i:t,:]\n",
    "        dW2 = (a1.T).dot(dz2)\n",
    "        db2 = np.sum(dz2,axis=0,keepdims=True)\n",
    "        da1 = dz2.dot(W2.T)\n",
    "        dz1 = da1*(1-np.power(a1,2))\n",
    "        dW1 = (X_t[i:t,:].T).dot(dz1)\n",
    "        db1 = np.sum(dz1,axis=0)\n",
    "        \n",
    "        # Adding the regulization\n",
    "        dW2 += reg_lambda*W2\n",
    "        dW1 += reg_lambda*W1\n",
    "        # gradient descent\n",
    "        W1 += -(1./num_example) *episilon*dW1\n",
    "        b1 += -(1./num_example)*episilon*db1\n",
    "        W2 += -(1./num_example)*episilon*dW2\n",
    "        b2 += -(1./num_example)*episilon*db2\n",
    "        \n",
    "        # Assigning the update parameters to the model\n",
    "        model = {'W1':W1,'b1':b1,'W2':W2,'b2':b2}\n",
    "        effective_loss.append(cal_loss(model,X_t,y_))\n",
    "        if print_agr and k % 1000 == 0:\n",
    "            print('The parameters are : W1',W1,'\\nW2',W2,'\\nb1',b1,'\\nb2',b2)\n",
    "        if print_loss and k % 1000 == 0:\n",
    "            print('The loss at ',k,' instance is :',cal_loss(model,X_t[i:t,:],y_[i:t,:]))\n",
    "        if i == 152:\n",
    "            i = 0\n",
    "        else:\n",
    "            i += batch_size\n",
    "        if i == 152:\n",
    "            t = i + 5\n",
    "        else:\n",
    "            t = i + batch_size\n",
    "    return model,np.array(effective_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a prediction functio  to get the prediction\n",
    "def predict(model,x):\n",
    "    W1,b1,W2,b2 = model['W1'],model['b1'],model['W2'],model['b2']\n",
    "    #print(W1)\n",
    "    #print(b1)\n",
    "    #print(W2)\n",
    "    #print(b2)\n",
    "    z1 = x.dot(W1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    a2 = np.exp(z2)\n",
    "    #print(a2)\n",
    "    prob = a2 / np.sum(a2,axis=1,keepdims=True)\n",
    "    #print(prob)\n",
    "    return np.argmax(prob,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define a loss function\n",
    "def cal_loss(model,x,y,reg = 0.01):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    # Forward propagation to calculate our predictions\n",
    "    z1 = x.dot(W1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    a2 = np.exp(z2)\n",
    "    num_examples = len(y)\n",
    "    prob = a2 / np.sum(a2,axis=1,keepdims=True)\n",
    "    cros_ent = -(np.multiply(y,np.log(prob))+np.multiply((1-y),np.log(1-prob)))\n",
    "    data_loss = np.sum(cros_ent)\n",
    "    # Add regulatization term to loss (optional)\n",
    "    data_loss += reg/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "    return 1./num_examples * data_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken is : 17.56701683998108\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.88      0.91        17\n",
      "          1       0.95      1.00      0.97        19\n",
      "          2       0.94      0.94      0.94        17\n",
      "\n",
      "avg / total       0.94      0.94      0.94        53\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f929f5a9160>]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f929f8b4710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We now train a 3 layer network that has 3 neurons in the hidden layer\\\n",
    "# We first started with 3 neurons in hidden layer, it was underfitting.\n",
    "# We then went to 8 neurons and regularized 0.1\n",
    "\"\"\" Note:\n",
    "        epoch 20000\n",
    "        Batch gradient descent \n",
    "        8 neurons\n",
    "        episilon - 0.015 lambda-0.2 acc - 95 94 94 val = 95 94 94\n",
    "        episilon - 0.01 lambda-0.2 acc - 95 94 94\n",
    "        episilon - 0.01 lambda-0.1 acc - 96 96 96 val = 95 94 94 (better)\n",
    "        episilon - 0.01 lambda-0.09 acc - 95 94 94\n",
    "        episilon - 0.01 lambda-0.12 acc - 96 96 96 val = 95 94 94\n",
    "        \n",
    "        epoch 25000\n",
    "        Batch gradient descent \n",
    "        8 neurons\n",
    "        episilon - 0.01 lambda-0.1 acc - 96 96 96 val = 95 94 94\n",
    "        \n",
    "        \n",
    "        epoch 30000\n",
    "        Batch gradient descent \n",
    "        8 neurons\n",
    "        episilon - 0.01 lambda-0.1 acc - 90 90 90 val = 94 58 70\n",
    "        Here over-shooting starts.\n",
    "        episilon - 0.01 lambda-0.09 acc - 94 94 94 val = 95 94 94\n",
    "        episilon - 0.01 lambda-0.08 acc - 96 96 96 val = 93 92 92\n",
    "        episilon - 0.01 lambda-0.07 acc - 97 97 97 val = 92 92 92\n",
    "        \n",
    "        epoch 20000\n",
    "        Batch gradient descent \n",
    "        7 neurons\n",
    "        episilon - 0.01 lambda-0.1 acc - 96 96 96 val = 94 58 70\n",
    "        \n",
    "        epoch 30000\n",
    "        Batch gradient descent \n",
    "        7 neurons\n",
    "        episilon - 0.01 lambda-0.08 acc - 96 96 96 val = 96 96 96\n",
    "        \n",
    "        epoch 20000\n",
    "        Batch gradient descent with momentum\n",
    "        10 neurons\n",
    "        episilon - 0.01 lambda-0.08 acc - 97 97 97 val = 96 96 96\n",
    "        12 neurons\n",
    "        episilon - 0.01 lambda-0.08 acc - 97 97 97 val = 96 96 96 (smoother)\n",
    "        episilon - 0.01 lambda-0.1 acc - 97 97 97 val = 96 96 96\n",
    "        \n",
    "        epoch 30000\n",
    "        Batch gradient descent with momentum\n",
    "        12 neurons\n",
    "        episilon - 0.02 lambda-0.1 acc - 99 99 99 val = 94 94 94\n",
    "        episilon - 0.02 lambda-0.2 acc - 99 99 99 val = 94 94 94\n",
    "        \n",
    "\"\"\"\n",
    "start = time.time()\n",
    "epoch = 30000\n",
    "model,loss = build_NN(12,episilon =0.03,epochs = epoch,reg_lambda = 0.25,norm = True,print_agr = False,print_loss = False)\n",
    "print('Time taken is :',time.time()-start)\n",
    "print(metrics.classification_report(predict(model,X_ts),y_ts))\n",
    "plt.plot(np.arange(epoch),loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken is : 6.660949945449829\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.64      0.78        25\n",
      "          1       0.65      1.00      0.79        13\n",
      "          2       0.88      1.00      0.94        15\n",
      "\n",
      "avg / total       0.88      0.83      0.83        53\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f929fbeef60>]"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f929fdc9fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    epoch 20000\n",
    "    Mini-Batch gradient descent \n",
    "    10 neurons\n",
    "    episilon - 0.01 lambda-0.08 acc - 87 76 79 val = 86 79 81\n",
    "    episilon - 0.008 lambda-0.08 acc - 86 75 78 val = 80 75 77\n",
    "    episilon - 0.01 lambda-0.1 acc - 87 76 79 val = 86 79 81\n",
    "    episilon - 0.01 lambda-0.2 acc - 87 76 79 val = 86 79 81\n",
    "    episilon - 0.05 lambda-0.2 acc - 92 91 91 val = 90 87 88\n",
    "    \n",
    "    \n",
    "    epoch 30000\n",
    "    Mini-Batch gradient descent \n",
    "    10 neurons\n",
    "    episilon - 0.01 lambda-0.2 acc - 88 81 82 val = 89 85 86\n",
    "   \n",
    "    epoch 40000\n",
    "    Mini-Batch gradient descent \n",
    "    10 neurons\n",
    "    episilon - 0.01 lambda-0.2 acc - 92 91 91 val = 90 87 88\n",
    "    episilon - 0.05 lambda-0.2 acc - 94 94 94 val = 85 81 81\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "\"\"\"\n",
    "\n",
    "start = time.time()\n",
    "epoch = 20000\n",
    "model,loss = build_NN_in_batch(8,episilon =0.1,epochs = epoch,reg_lambda = 0.3,norm = True,print_agr = False,print_loss = False)\n",
    "print('Time taken is :',time.time()-start)\n",
    "print(metrics.classification_report(predict(model,X_ts),y_ts))\n",
    "plt.plot(np.arange(epoch),loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 2, 1, 1, 0, 2, 0, 1, 0, 2, 2, 0, 1, 0, 2, 2, 0, 2, 0, 0, 2,\n",
       "       0, 0, 2, 2, 2, 1, 2, 1, 0, 0, 0, 2, 0, 0, 0, 2, 1, 1, 2, 2, 2, 1, 1,\n",
       "       2, 1, 1, 0, 2, 0, 0, 2, 1, 0, 2, 0, 2, 0, 2, 2, 1, 2, 2, 0, 2, 2, 2,\n",
       "       0, 2, 2, 0, 0, 1, 1, 1, 2, 0, 0, 0, 2, 1, 2, 1, 0, 0, 1, 2, 1, 1, 2,\n",
       "       0, 2, 2, 1, 0, 1, 2, 1, 2, 1, 0, 2, 2, 1, 1, 2, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 2, 1, 0, 0, 1, 1, 0, 2, 2, 2, 2, 2, 0, 2, 1, 0, 0, 1, 2, 0,\n",
       "       2, 0, 0, 0, 2, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 1, 2, 1, 1])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(model,X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 0, 1, 0, 2, 2, 0, 2, 1, 1, 2,\n",
       "       0, 0, 2, 0, 2, 1, 2, 0, 1, 1, 1, 2, 0, 2, 0, 2, 0, 1, 2, 2, 2, 1, 2,\n",
       "       2, 1, 1, 0, 2, 0, 1, 2, 1, 0, 2, 0, 2, 0, 2, 2, 0, 2, 2, 0, 2, 2, 2,\n",
       "       1, 2, 2, 0, 1, 0, 1, 0, 2, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 2, 0, 0, 2,\n",
       "       0, 2, 2, 1, 0, 0, 2, 1, 2, 1, 1, 2, 2, 1, 0, 2, 0, 1, 1, 0, 0, 1, 1,\n",
       "       0, 1, 1, 2, 1, 0, 1, 0, 1, 0, 2, 2, 2, 0, 2, 1, 2, 1, 0, 1, 0, 2, 1,\n",
       "       0, 0, 1, 0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 1, 2, 1, 1])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
