{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# i am trask \n",
    "- https://iamtrask.github.io/2015/07/28/dropout/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Just Give Me The Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n",
    "y = np.array([[0,1,1,0]]).T\n",
    "alpha,hidden_dim,dropout_percent,do_dropout = (0.5,4,0.2,True)\n",
    "synapse_0 = 2*np.random.random((3,hidden_dim)) - 1\n",
    "synapse_1 = 2*np.random.random((hidden_dim,1)) - 1\n",
    "for j in range(60000):\n",
    "    layer_1 = (1/(1+np.exp(-(np.dot(X,synapse_0)))))\n",
    "    if(do_dropout):\n",
    "        layer_1 *= np.random.binomial([np.ones((len(X),hidden_dim))],1-dropout_percent)[0] * (1.0/(1-dropout_percent))\n",
    "    layer_2 = 1/(1+np.exp(-(np.dot(layer_1,synapse_1))))\n",
    "    layer_2_delta = (layer_2 - y)*(layer_2*(1-layer_2))\n",
    "    layer_1_delta = layer_2_delta.dot(synapse_1.T) * (layer_1 * (1-layer_1))\n",
    "    synapse_1 -= (alpha * layer_1.T.dot(layer_2_delta))\n",
    "    synapse_0 -= (alpha * X.T.dot(layer_1_delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Part 1: What is Dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://iamtrask.github.io/img/sgd_randomness_ensemble.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Consider the graphic above from the previous post. The line represents the error the network generates for every value of a particular weight. The low-points (READ: low error) in that line signify the weight \"finding\" points of correlation between the input and output data. The balls in the picture signify various weights. They are trying to find those low points.\n",
    "\n",
    "- Consider the color. The ball's initial positions are randomly generated (just like weights in a neural network). If two balls randomly start in the same colored zone, they will converge to the same point. This makes them redundant! They're wasting computation and memory! This is exactly what happens in neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Why Dropout:** Dropout helps prevent weights from converging to identical positions. It does this by randomly turning nodes off when forward propagating. It then back-propagates with all the nodes turned on. Letâ€™s take a closer look.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hinton advocates tuning dropout in conjunction with tuning the size of your hidden layer. Increase your hidden layer size(s) with dropout turned off until you perfectly fit your data. Then, using the same hidden layer size, train with dropout turned on. This should be a nearly optimal configuration. Turn off dropout as soon as you're done training and voila! You have a working neural network!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
