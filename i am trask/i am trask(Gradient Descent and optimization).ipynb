{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# i am trask\n",
    "- https://iamtrask.github.io/2015/07/27/python-network-part2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just Give Me The Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n",
    "y = np.array([[0,1,1,0]]).T\n",
    "alpha,hidden_dim = (0.5,4)\n",
    "synapse_0 = 2*np.random.random((3,hidden_dim)) - 1\n",
    "synapse_1 = 2*np.random.random((hidden_dim,1)) - 1\n",
    "for j in xrange(60000):\n",
    "    layer_1 = 1/(1+np.exp(-(np.dot(X,synapse_0))))\n",
    "    layer_2 = 1/(1+np.exp(-(np.dot(layer_1,synapse_1))))\n",
    "    layer_2_delta = (layer_2 - y)*(layer_2*(1-layer_2))\n",
    "    layer_1_delta = layer_2_delta.dot(synapse_1.T) * (layer_1 * (1-layer_1))\n",
    "    synapse_1 -= (alpha * layer_1.T.dot(layer_2_delta))\n",
    "    synapse_0 -= (alpha * X.T.dot(layer_1_delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The takeaway here is that backpropagation doesn't optimize! It moves the error information from the end of the network to all the weights inside the network so that a different algorithm can optimize those weights to fit our data.\n",
    "\n",
    "__**A Few Optimization Methods:**__\n",
    "- Annealing\n",
    "- Stochastic Gradient Descent\n",
    "- AW-SGD (new!)\n",
    "- Momentum (SGD)\n",
    "- Nesterov Momentum (SGD)\n",
    "- AdaGrad\n",
    "- AdaDelta\n",
    "- ADAM\n",
    "- BFGS\n",
    "- LBFGS \n",
    "\n",
    "__**Visualizing the Difference:**__\n",
    "- ConvNet.js\n",
    "- RobertsDionne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://iamtrask.github.io/img/sgd_high.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sometimes the slope is so steep that we overshoot by a lot. Overshooting by a little is ok, but sometimes we overshoot by so much that we're even farther away than we started!\n",
    "- overshooting this far means we land at an EVEN STEEPER slope in the opposite direction. This causes us to overshoot again EVEN FARTHER. This viscious cycle of overshooting leading to more overshooting is called **divergence**.\n",
    "- If our gradients are too big, we make them smaller! We do this by multiplying them (all of them) by a single number between 0 and 1 (such as 0.01). This fraction is typically a single float called **alpha**. When we do this, we don't overshoot and our network converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://iamtrask.github.io/img/sgd_optimal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Problem 2: Local Minimums\n",
    "    - Sometimes your bucket has a funny shape, and following the slope doesn't take you to the absolute lowest point. \n",
    "    ![alt text](https://iamtrask.github.io/img/sgd_local_min.png)\n",
    "    - #### Solution 2: Multiple Random Starting States\n",
    "        - Imagine that we randomly placed 100 balls on this line and started optimizing all of them. If we did so, they would all end up in only 5 positions, mapped out by the five colored balls above. The colored regions represent the domain of each local minimum. This is far better than pure random searching, which has to randomly try EVERY space.\n",
    "        ![alt text](https://iamtrask.github.io/img/sgd_randomness_ensemble.png)\n",
    "        - **In Neural Networks:** One way that neural networks accomplish this is by having very large hidden layers. You see, each hidden node in a layer starts out in a different random starting state(initially each layer has its weights initialized randomly). This allows each hidden node to converge to different patterns in the network. Parameterizing this size allows the neural network user to potentially try thousands (or tens of billions) of different local minima in a single neural network.\n",
    "        - **Sidenote 2:** A close eye might ask, \"Well, why would we allow a lot of nodes to converge to the same spot? That's actually wasting computational power!\" That's an excellent point. The current state-of-the-art approaches to avoiding hidden nodes coming up with the same answer (by searching the same space) are Dropout and Drop-Connect, which I intend to cover in a later post.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Part 4: SGD in Neural Networks\n",
    "    - **So at this point you might be wondering, how does this relate to neural networks and backpropagation?**\n",
    "    - *Let's take a look at what this process looks like in a simple 2 layer neural network.*\n",
    "    - ![alt text](https://iamtrask.github.io/img/sgd_local_min.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Layer Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output After Training:\n",
      "[[ 0.00505119]\n",
      " [ 0.00505119]\n",
      " [ 0.99494905]\n",
      " [ 0.99494905]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    "    \n",
    "# input dataset\n",
    "X = np.array([  [0,1],\n",
    "                [0,1],\n",
    "                [1,0],\n",
    "                [1,0] ])\n",
    "    \n",
    "# output dataset            \n",
    "y = np.array([[0,0,1,1]]).T\n",
    "\n",
    "# seed random numbers to make calculation\n",
    "# deterministic (just a good practice)\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize weights randomly with mean 0\n",
    "synapse_0 = 2*np.random.random((2,1)) - 1\n",
    "\n",
    "for iter in range(10000):\n",
    "\n",
    "    # forward propagation\n",
    "    layer_0 = X\n",
    "    layer_1 = sigmoid(np.dot(layer_0,synapse_0))\n",
    "\n",
    "    # how much did we miss?\n",
    "    layer_1_error = layer_1 - y\n",
    "\n",
    "    # multiply how much we missed by the \n",
    "    # slope of the sigmoid at the values in l1\n",
    "    layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)\n",
    "    synapse_0_derivative = np.dot(layer_0.T,layer_1_delta)\n",
    "\n",
    "    # update weights\n",
    "    synapse_0 -= synapse_0_derivative\n",
    "\n",
    "print(\"Output After Training:\")\n",
    "print(layer_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, in this case, we have a single error at the output (single value), which is computed on line 35. Since we have 2 weights, the output \"error plane\" is a 3 dimensional space. We can think of this as an (x,y,z) plane, where vertical is the error, and x and y are the values of our two weights in syn0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://iamtrask.github.io/img/3d_error_plane.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Improvement 1: Adding and Tuning the Alpha Parameter\n",
    "    - What is Alpha? As described above, the alpha parameter reduces the size of each iteration's update in the simplest way possible. At the very last minute, right before we update the weights, we multiply the weight update by alpha (usually between 0 and 1, thus reducing the size of the weight update). This tiny change to the code has absolutely massive impact on its ability to train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we add the alpha parameter in the weight updation step of oue 3 layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training With Alpha:0.001\n",
      "Error after 0 iterations:0.496410031903\n",
      "Error after 10000 iterations:0.495164025493\n",
      "Error after 20000 iterations:0.493596043188\n",
      "Error after 30000 iterations:0.491606358559\n",
      "Error after 40000 iterations:0.489100166544\n",
      "Error after 50000 iterations:0.485977857846\n",
      "\n",
      "Training With Alpha:0.01\n",
      "Error after 0 iterations:0.496410031903\n",
      "Error after 10000 iterations:0.457431074442\n",
      "Error after 20000 iterations:0.359097202563\n",
      "Error after 30000 iterations:0.239358137159\n",
      "Error after 40000 iterations:0.143070659013\n",
      "Error after 50000 iterations:0.0985964298089\n",
      "\n",
      "Training With Alpha:0.1\n",
      "Error after 0 iterations:0.496410031903\n",
      "Error after 10000 iterations:0.0428880170001\n",
      "Error after 20000 iterations:0.0240989942285\n",
      "Error after 30000 iterations:0.0181106521468\n",
      "Error after 40000 iterations:0.0149876162722\n",
      "Error after 50000 iterations:0.0130144905381\n",
      "\n",
      "Training With Alpha:1\n",
      "Error after 0 iterations:0.496410031903\n",
      "Error after 10000 iterations:0.00858452565325\n",
      "Error after 20000 iterations:0.00578945986251\n",
      "Error after 30000 iterations:0.00462917677677\n",
      "Error after 40000 iterations:0.00395876528027\n",
      "Error after 50000 iterations:0.00351012256786\n",
      "\n",
      "Training With Alpha:10\n",
      "Error after 0 iterations:0.496410031903\n",
      "Error after 10000 iterations:0.00312938876301\n",
      "Error after 20000 iterations:0.00214459557985\n",
      "Error after 30000 iterations:0.00172397549956\n",
      "Error after 40000 iterations:0.00147821451229\n",
      "Error after 50000 iterations:0.00131274062834\n",
      "\n",
      "Training With Alpha:100\n",
      "Error after 0 iterations:0.496410031903\n",
      "Error after 10000 iterations:0.125476983855\n",
      "Error after 20000 iterations:0.125330333528\n",
      "Error after 30000 iterations:0.125267728765\n",
      "Error after 40000 iterations:0.12523107366\n",
      "Error after 50000 iterations:0.125206352755\n",
      "\n",
      "Training With Alpha:1000\n",
      "Error after 0 iterations:0.496410031903\n",
      "Error after 10000 iterations:0.5\n",
      "Error after 20000 iterations:0.5\n",
      "Error after 30000 iterations:0.5\n",
      "Error after 40000 iterations:0.5\n",
      "Error after 50000 iterations:0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "alphas = [0.001,0.01,0.1,1,10,100,1000]\n",
    "\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    "    \n",
    "X = np.array([[0,0,1],\n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [1,1,1]])\n",
    "                \n",
    "y = np.array([[0],\n",
    "\t\t\t[1],\n",
    "\t\t\t[1],\n",
    "\t\t\t[0]])\n",
    "\n",
    "for alpha in alphas:\n",
    "    print(\"\\nTraining With Alpha:\" + str(alpha))\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # randomly initialize our weights with mean 0\n",
    "    synapse_0 = 2*np.random.random((3,4)) - 1\n",
    "    synapse_1 = 2*np.random.random((4,1)) - 1\n",
    "\n",
    "    for j in range(60000):\n",
    "\n",
    "        # Feed forward through layers 0, 1, and 2\n",
    "        layer_0 = X\n",
    "        layer_1 = sigmoid(np.dot(layer_0,synapse_0))\n",
    "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
    "\n",
    "        # how much did we miss the target value?\n",
    "        layer_2_error = layer_2 - y\n",
    "\n",
    "        if (j% 10000) == 0:\n",
    "            print(\"Error after \"+str(j)+\" iterations:\" + str(np.mean(np.abs(layer_2_error))))\n",
    "\n",
    "        # in what direction is the target value?\n",
    "        # were we really sure? if so, don't change too much.\n",
    "        layer_2_delta = layer_2_error*sigmoid_output_to_derivative(layer_2)\n",
    "\n",
    "        # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "        layer_1_error = layer_2_delta.dot(synapse_1.T)\n",
    "\n",
    "        # in what direction is the target l1?\n",
    "        # were we really sure? if so, don't change too much.\n",
    "        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)\n",
    "\n",
    "        synapse_1 -= alpha * (layer_1.T.dot(layer_2_delta))\n",
    "        synapse_0 -= alpha * (layer_0.T.dot(layer_1_delta))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### So, what did we observe with the different alpha sizes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training With Alpha:0.001\n",
      "Error:0.496410031903\n",
      "Error:0.495164025493\n",
      "Error:0.493596043188\n",
      "Error:0.491606358559\n",
      "Error:0.489100166544\n",
      "Error:0.485977857846\n",
      "Synapse 0\n",
      "[[-0.28448441  0.32471214 -1.53496167 -0.47594822]\n",
      " [-0.7550616  -1.04593014 -1.45446052 -0.32606771]\n",
      " [-0.2594825  -0.13487028 -0.29722666  0.40028038]]\n",
      "Synapse 0 Update Direction Changes\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 1.  0.  1.  1.]]\n",
      "Synapse 1\n",
      "[[-0.61957526]\n",
      " [ 0.76414675]\n",
      " [-1.49797046]\n",
      " [ 0.40734574]]\n",
      "Synapse 1 Update Direction Changes\n",
      "[[ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]]\n",
      "\n",
      "Training With Alpha:0.01\n",
      "Error:0.496410031903\n",
      "Error:0.457431074442\n",
      "Error:0.359097202563\n",
      "Error:0.239358137159\n",
      "Error:0.143070659013\n",
      "Error:0.0985964298089\n",
      "Synapse 0\n",
      "[[ 2.39225985  2.56885428 -5.38289334 -3.29231397]\n",
      " [-0.35379718 -4.6509363  -5.67005693 -1.74287864]\n",
      " [-0.15431323 -1.17147894  1.97979367  3.44633281]]\n",
      "Synapse 0 Update Direction Changes\n",
      "[[ 1.  1.  0.  0.]\n",
      " [ 2.  0.  0.  2.]\n",
      " [ 4.  2.  1.  1.]]\n",
      "Synapse 1\n",
      "[[-3.70045078]\n",
      " [ 4.57578637]\n",
      " [-7.63362462]\n",
      " [ 4.73787613]]\n",
      "Synapse 1 Update Direction Changes\n",
      "[[ 2.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]]\n",
      "\n",
      "Training With Alpha:0.1\n",
      "Error:0.496410031903\n",
      "Error:0.0428880170001\n",
      "Error:0.0240989942285\n",
      "Error:0.0181106521468\n",
      "Error:0.0149876162722\n",
      "Error:0.0130144905381\n",
      "Synapse 0\n",
      "[[ 3.88035459  3.6391263  -5.99509098 -3.8224267 ]\n",
      " [-1.72462557 -5.41496387 -6.30737281 -3.03987763]\n",
      " [ 0.45953952 -1.77301389  2.37235987  5.04309824]]\n",
      "Synapse 0 Update Direction Changes\n",
      "[[ 1.  1.  0.  0.]\n",
      " [ 2.  0.  0.  2.]\n",
      " [ 4.  2.  1.  1.]]\n",
      "Synapse 1\n",
      "[[-5.72386389]\n",
      " [ 6.15041318]\n",
      " [-9.40272079]\n",
      " [ 6.61461026]]\n",
      "Synapse 1 Update Direction Changes\n",
      "[[ 2.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]]\n",
      "\n",
      "Training With Alpha:1\n",
      "Error:0.496410031903\n",
      "Error:0.00858452565325\n",
      "Error:0.00578945986251\n",
      "Error:0.00462917677677\n",
      "Error:0.00395876528027\n",
      "Error:0.00351012256786\n",
      "Synapse 0\n",
      "[[ 4.6013571   4.17197193 -6.30956245 -4.19745118]\n",
      " [-2.58413484 -5.81447929 -6.60793435 -3.68396123]\n",
      " [ 0.97538679 -2.02685775  2.52949751  5.84371739]]\n",
      "Synapse 0 Update Direction Changes\n",
      "[[ 1.  1.  0.  0.]\n",
      " [ 2.  0.  0.  2.]\n",
      " [ 4.  2.  1.  1.]]\n",
      "Synapse 1\n",
      "[[ -6.96765763]\n",
      " [  7.14101949]\n",
      " [-10.31917382]\n",
      " [  7.86128405]]\n",
      "Synapse 1 Update Direction Changes\n",
      "[[ 2.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]]\n",
      "\n",
      "Training With Alpha:10\n",
      "Error:0.496410031903\n",
      "Error:0.00312938876301\n",
      "Error:0.00214459557985\n",
      "Error:0.00172397549956\n",
      "Error:0.00147821451229\n",
      "Error:0.00131274062834\n",
      "Synapse 0\n",
      "[[ 4.52597806  5.77663165 -7.34266481 -5.29379829]\n",
      " [ 1.66715206 -7.16447274 -7.99779235 -1.81881849]\n",
      " [-4.27032921 -3.35838279  3.44594007  4.88852208]]\n",
      "Synapse 0 Update Direction Changes\n",
      "[[  7.  19.   2.   6.]\n",
      " [  7.   2.   0.  22.]\n",
      " [ 19.  26.   9.  17.]]\n",
      "Synapse 1\n",
      "[[ -8.58485788]\n",
      " [ 10.1786297 ]\n",
      " [-14.87601886]\n",
      " [  7.57026121]]\n",
      "Synapse 1 Update Direction Changes\n",
      "[[ 22.]\n",
      " [ 15.]\n",
      " [  4.]\n",
      " [ 15.]]\n",
      "\n",
      "Training With Alpha:100\n",
      "Error:0.496410031903\n",
      "Error:0.125476983855\n",
      "Error:0.125330333528\n",
      "Error:0.125267728765\n",
      "Error:0.12523107366\n",
      "Error:0.125206352755\n",
      "Synapse 0\n",
      "[[-17.20515374   1.89881432 -16.95533155  -8.23482697]\n",
      " [  5.70240666 -17.23785162  -9.48052575  -7.92972576]\n",
      " [ -4.18781711  -0.3388181    2.82024767  -8.40059859]]\n",
      "Synapse 0 Update Direction Changes\n",
      "[[  8.   7.   3.   2.]\n",
      " [ 13.   8.   2.   4.]\n",
      " [ 16.  13.  12.   8.]]\n",
      "Synapse 1\n",
      "[[  9.6828537 ]\n",
      " [  9.55731915]\n",
      " [-16.03907021]\n",
      " [  6.27326973]]\n",
      "Synapse 1 Update Direction Changes\n",
      "[[ 13.]\n",
      " [ 11.]\n",
      " [ 12.]\n",
      " [ 10.]]\n",
      "\n",
      "Training With Alpha:1000\n",
      "Error:0.496410031903\n",
      "Error:0.5\n",
      "Error:0.5\n",
      "Error:0.5\n",
      "Error:0.5\n",
      "Error:0.5\n",
      "Synapse 0\n",
      "[[-56.06177241  -4.66409623  -5.65196179 -23.05868769]\n",
      " [ -4.52271708  -4.78184499 -10.88770202 -15.85879101]\n",
      " [-89.56678495  10.81119741  37.02351518 -48.33299795]]\n",
      "Synapse 0 Update Direction Changes\n",
      "[[ 3.  2.  4.  1.]\n",
      " [ 1.  2.  2.  1.]\n",
      " [ 6.  6.  4.  1.]]\n",
      "Synapse 1\n",
      "[[  25.16188889]\n",
      " [  -8.68235535]\n",
      " [-116.60053379]\n",
      " [  11.41582458]]\n",
      "Synapse 1 Update Direction Changes\n",
      "[[ 7.]\n",
      " [ 7.]\n",
      " [ 7.]\n",
      " [ 3.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "alphas = [0.001,0.01,0.1,1,10,100,1000]\n",
    "\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    "    \n",
    "X = np.array([[0,0,1],\n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [1,1,1]])\n",
    "                \n",
    "y = np.array([[0],\n",
    "\t\t\t[1],\n",
    "\t\t\t[1],\n",
    "\t\t\t[0]])\n",
    "\n",
    "\n",
    "\n",
    "for alpha in alphas:\n",
    "    print(\"\\nTraining With Alpha:\" + str(alpha))\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # randomly initialize our weights with mean 0\n",
    "    synapse_0 = 2*np.random.random((3,4)) - 1\n",
    "    synapse_1 = 2*np.random.random((4,1)) - 1\n",
    "        \n",
    "    prev_synapse_0_weight_update = np.zeros_like(synapse_0)\n",
    "    prev_synapse_1_weight_update = np.zeros_like(synapse_1)\n",
    "\n",
    "    synapse_0_direction_count = np.zeros_like(synapse_0)\n",
    "    synapse_1_direction_count = np.zeros_like(synapse_1)\n",
    "        \n",
    "    for j in range(60000):\n",
    "\n",
    "        # Feed forward through layers 0, 1, and 2\n",
    "        layer_0 = X\n",
    "        layer_1 = sigmoid(np.dot(layer_0,synapse_0))\n",
    "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
    "\n",
    "        # how much did we miss the target value?\n",
    "        layer_2_error = y - layer_2\n",
    "\n",
    "        if (j% 10000) == 0:\n",
    "            print(\"Error:\" + str(np.mean(np.abs(layer_2_error))))\n",
    "\n",
    "        # in what direction is the target value?\n",
    "        # were we really sure? if so, don't change too much.\n",
    "        layer_2_delta = layer_2_error*sigmoid_output_to_derivative(layer_2)\n",
    "\n",
    "        # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "        layer_1_error = layer_2_delta.dot(synapse_1.T)\n",
    "\n",
    "        # in what direction is the target l1?\n",
    "        # were we really sure? if so, don't change too much.\n",
    "        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)\n",
    "        \n",
    "        synapse_1_weight_update = (layer_1.T.dot(layer_2_delta))\n",
    "        synapse_0_weight_update = (layer_0.T.dot(layer_1_delta))\n",
    "        \n",
    "        if(j > 0):\n",
    "            synapse_0_direction_count += np.abs(((synapse_0_weight_update > 0)+0) - ((prev_synapse_0_weight_update > 0) + 0))\n",
    "            synapse_1_direction_count += np.abs(((synapse_1_weight_update > 0)+0) - ((prev_synapse_1_weight_update > 0) + 0))        \n",
    "        \n",
    "        synapse_1 += alpha * synapse_1_weight_update\n",
    "        synapse_0 += alpha * synapse_0_weight_update\n",
    "        \n",
    "        prev_synapse_0_weight_update = synapse_0_weight_update\n",
    "        prev_synapse_1_weight_update = synapse_1_weight_update\n",
    "    \n",
    "    print(\"Synapse 0\")\n",
    "    print(synapse_0)\n",
    "    \n",
    "    print(\"Synapse 0 Update Direction Changes\")\n",
    "    print(synapse_0_direction_count)\n",
    "    \n",
    "    print(\"Synapse 1\")\n",
    "    print(synapse_1)\n",
    "\n",
    "    print(\"Synapse 1 Update Direction Changes\")\n",
    "    print(synapse_1_direction_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Improvement 2: Parameterizing the Size of the Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training With Alpha:0.001\n",
      "Error after 0 iterations:0.496439922501\n",
      "Error after 10000 iterations:0.491049468129\n",
      "Error after 20000 iterations:0.484976307027\n",
      "Error after 30000 iterations:0.477830678793\n",
      "Error after 40000 iterations:0.46903846539\n",
      "Error after 50000 iterations:0.458029258565\n",
      "\n",
      "Training With Alpha:0.01\n",
      "Error after 0 iterations:0.496439922501\n",
      "Error after 10000 iterations:0.356379061648\n",
      "Error after 20000 iterations:0.146939845465\n",
      "Error after 30000 iterations:0.0880156127416\n",
      "Error after 40000 iterations:0.065147819275\n",
      "Error after 50000 iterations:0.0529658087026\n",
      "\n",
      "Training With Alpha:0.1\n",
      "Error after 0 iterations:0.496439922501\n",
      "Error after 10000 iterations:0.0305404908386\n",
      "Error after 20000 iterations:0.0190638725334\n",
      "Error after 30000 iterations:0.0147643907296\n",
      "Error after 40000 iterations:0.0123892429905\n",
      "Error after 50000 iterations:0.0108421669738\n",
      "\n",
      "Training With Alpha:1\n",
      "Error after 0 iterations:0.496439922501\n",
      "Error after 10000 iterations:0.00736052234249\n",
      "Error after 20000 iterations:0.00497251705039\n",
      "Error after 30000 iterations:0.00396863978159\n",
      "Error after 40000 iterations:0.00338641021983\n",
      "Error after 50000 iterations:0.00299625684932\n",
      "\n",
      "Training With Alpha:10\n",
      "Error after 0 iterations:0.496439922501\n",
      "Error after 10000 iterations:0.00223965528737\n",
      "Error after 20000 iterations:0.0015323482297\n",
      "Error after 30000 iterations:0.00123300005586\n",
      "Error after 40000 iterations:0.0010582588911\n",
      "Error after 50000 iterations:0.00094053801417\n",
      "\n",
      "Training With Alpha:100\n",
      "Error after 0 iterations:0.496439922501\n",
      "Error after 10000 iterations:0.5\n",
      "Error after 20000 iterations:0.5\n",
      "Error after 30000 iterations:0.5\n",
      "Error after 40000 iterations:0.5\n",
      "Error after 50000 iterations:0.5\n",
      "\n",
      "Training With Alpha:1000\n",
      "Error after 0 iterations:0.496439922501\n",
      "Error after 10000 iterations:0.5\n",
      "Error after 20000 iterations:0.5\n",
      "Error after 30000 iterations:0.5\n",
      "Error after 40000 iterations:0.5\n",
      "Error after 50000 iterations:0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "alphas = [0.001,0.01,0.1,1,10,100,1000]\n",
    "hiddenSize = 32\n",
    "\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    "    \n",
    "X = np.array([[0,0,1],\n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [1,1,1]])\n",
    "                \n",
    "y = np.array([[0],\n",
    "\t\t\t[1],\n",
    "\t\t\t[1],\n",
    "\t\t\t[0]])\n",
    "\n",
    "for alpha in alphas:\n",
    "    print(\"\\nTraining With Alpha:\" + str(alpha))\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # randomly initialize our weights with mean 0\n",
    "    synapse_0 = 2*np.random.random((3,hiddenSize)) - 1\n",
    "    synapse_1 = 2*np.random.random((hiddenSize,1)) - 1\n",
    "\n",
    "    for j in range(60000):\n",
    "\n",
    "        # Feed forward through layers 0, 1, and 2\n",
    "        layer_0 = X\n",
    "        layer_1 = sigmoid(np.dot(layer_0,synapse_0))\n",
    "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
    "\n",
    "        # how much did we miss the target value?\n",
    "        layer_2_error = layer_2 - y\n",
    "\n",
    "        if (j% 10000) == 0:\n",
    "            print(\"Error after \"+str(j)+\" iterations:\" + str(np.mean(np.abs(layer_2_error))))\n",
    "\n",
    "        # in what direction is the target value?\n",
    "        # were we really sure? if so, don't change too much.\n",
    "        layer_2_delta = layer_2_error*sigmoid_output_to_derivative(layer_2)\n",
    "\n",
    "        # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "        layer_1_error = layer_2_delta.dot(synapse_1.T)\n",
    "\n",
    "        # in what direction is the target l1?\n",
    "        # were we really sure? if so, don't change too much.\n",
    "        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)\n",
    "\n",
    "        synapse_1 -= alpha * (layer_1.T.dot(layer_2_delta))\n",
    "        synapse_0 -= alpha * (layer_0.T.dot(layer_1_delta))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
